{
  
    
        "post0": {
            "title": "Metadata visualization",
            "content": "The coswara database is created by crowdsourcing respiratory sound samples. In this post we will visualize the crowd (or participant) distribution along certain dimensions collected in metadata questionaire. We will read the CSV file containing the metadata information of all the users (as on 07 August 2020). From the whole dataset, we have manually listened to 941 participants&#39; audio samples. Below we present this data. A more detailed documentation is also available here and will be presented at the Interspeech 2020 conference. . First we visualize the gender, age, and country-wise (India/outside) distribution. . #collapse_show # import some packages import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) # load CSV file fname = &#39;combined_plus_annotated_IS2020.csv&#39; DF = pd.read_csv(&#39;./my_data/&#39;+fname) # plot gender information gender_labels = DF[&#39;g&#39;].unique() gender_cnt = [] for i in range(len(gender_labels)): gender_cnt.append(len(DF[(DF[&#39;g&#39;] == gender_labels[i]) &amp; DF[&#39;cough-heavy-quality&#39;]])) fig = plt.subplots(figsize=(16, 4)) ax = plt.subplot(1,3,1) ax.bar(2,gender_cnt[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) ax.bar(4,gender_cnt[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot;//&quot;,color=&#39;blue&#39;,width=.5) for i, v in enumerate(gender_cnt): ax.text(2*(i+1)-.2,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) plt.xticks([2,4], [&#39;MALE&#39;,&#39;FEMALE&#39;],rotation=0) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) # ax.set_xlim(0,5) # ax.set_ylim(200,1500) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plot age information age_labels = DF[&#39;a&#39;].unique() age_cnt = [] for i in range(len(age_labels)): age_cnt.append(len(DF[(DF[&#39;a&#39;] == age_labels[i])])) ax = plt.subplot(1,3,2) ax.bar(age_labels,age_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,color=&#39;blue&#39;) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xlabel(&#39;AGE&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plot country information country_labels = DF[&#39;l_c&#39;].unique() country_cnt = [] for i in range(len(country_labels)): country_cnt.append(len(DF[DF[&#39;l_c&#39;] == country_labels[i]])) country_cnt = np.array(country_cnt) indx = np.argsort(country_cnt)[::-1] country_cnt = country_cnt[indx] country_labels = country_labels[indx] two_categories = [country_cnt[0],np.sum(country_cnt[1:])] two_labels = [&#39;INDIA&#39;,&#39;OTHERS&#39;] ax = plt.subplot(1,3,3) ax.bar(2,two_categories[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) ax.bar(4,two_categories[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) plt.xticks([2,4],two_labels,rotation=0) for i, v in enumerate(two_categories): ax.text(2*(i+1)-.25,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . India has 29 states. Next, lets visualize the across state distribution. We show only top 5 below. . #collapse_show state_labels = DF[&#39;l_s&#39;].unique() state_cnt = [] for i in range(len(state_labels)): state_cnt.append(len(DF[DF[&#39;l_s&#39;] == state_labels[i]])) state_cnt = np.array(state_cnt) indx = np.argsort(state_cnt)[::-1][0:6] state_cnt = state_cnt[indx] state_labels = state_labels[indx] fig, ax = plt.subplots(figsize=(8, 4)) ax.bar(np.arange(0,len(state_cnt)),state_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.25) ax.set_ylabel(&#39;PARTICIPANT COUNT&#39;,fontsize=14) # # ax.text(1.5,-9,&#39;MEAN&#39;,horizontalalignment=&#39;center&#39;) plt.xticks(np.arange(0,len(state_cnt)),state_labels,rotation=30,fontsize=13) for i, v in enumerate(state_cnt): ax.text(i-.15,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . Next, we group the participants into two groups, healthy and unhealthy. This grouping is based on absence/presence of any respiratory ailments reported in the metadata questionaire. . #collapse_show labels = [&#39;HEALTHY&#39;,&#39;UNHEALTHY&#39;] category_cnt = [] category_cnt.append(len(DF.loc[(DF[&#39;covid_status&#39;]==&#39;healthy&#39;) &amp; (DF[&#39;asthma&#39;]!=True)&amp; (DF[&#39;cld&#39;]!=True)&amp; (DF[&#39;cold&#39;]!=True)&amp; (DF[&#39;cough&#39;]!=True)&amp; (DF[&#39;pneumonia&#39;]!=True)&amp; (DF[&#39;fever&#39;]!=True)])) category_cnt.append(len(DF.loc[(DF[&#39;covid_status&#39;]==&#39;resp_illness_not_identified&#39;) | (DF[&#39;covid_status&#39;]==&#39;positive_mild&#39;) | (DF[&#39;asthma&#39;]==True)| (DF[&#39;cld&#39;]==True)| (DF[&#39;cold&#39;]==True)| (DF[&#39;cough&#39;]==True)| (DF[&#39;pneumonia&#39;]==True)| (DF[&#39;fever&#39;]==True)])) fig = plt.subplots(figsize=(4,4)) ax = plt.subplot(1,1,1) ax.bar(2,category_cnt[0],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5,label=&#39;clean&#39;) ax.bar(4,category_cnt[1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5,label=&#39;noisy&#39;) plt.xticks([2,4],labels,rotation=0) for i, v in enumerate(category_cnt): ax.text(2*(i+1)-.25,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() plt.show() . . Lastly, we have listened to all the 941 participants&#39; audio data and classified every audio file into clean, noisy, and bad (highly degraded). The resulting distribution across the nine sound categories is shown below. . #collapse_show labels = [&#39;vowel-o&#39;,&#39;vowel-e&#39;,&#39;vowel-a&#39;,&#39;cough-shallow&#39;,&#39;cough-heavy&#39;,&#39;breathing-shallow&#39;,&#39;breathing-deep&#39;, &#39;counting-normal&#39;,&#39;counting-fast&#39;] category_cnt = [] for label in labels: category_cnt.append(len(DF[(DF[label]==label) &amp; ((DF[label+&#39;-quality&#39;]==&#39;clean_audio&#39;)) &amp; ((DF[label+&#39;-cont&#39;]==&#39;y&#39;)) &amp; (DF[label+&#39;-vol&#39;]==&#39;y&#39;)])) category_cnt.append(len(DF[(DF[label]==label) &amp; ((DF[label+&#39;-quality&#39;]==&#39;noisy_audio&#39;)) &amp; ((DF[label+&#39;-cont&#39;]==&#39;y&#39;))])) category_cnt.append(len(DF[(DF[label]==label) &amp; (((DF[label+&#39;-quality&#39;]==&#39;bad_audio&#39;)) |((DF[label+&#39;-quality&#39;]==&#39;clean_audio&#39;)&amp;(DF[label+&#39;-cont&#39;]==&#39;n&#39;)) |((DF[label+&#39;-quality&#39;]==&#39;noisy_audio&#39;)&amp;(DF[label+&#39;-cont&#39;]==&#39;n&#39;)) )])) fig = plt.subplots(figsize=(10,4)) ax = plt.subplot(1,1,1) cnt = 0 indx = 0 xticks = [] for i in range(len(category_cnt)//3): if i ==0: ax.bar(cnt,category_cnt[indx],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=1,label=&#39;clean&#39;) ax.bar(cnt+1,category_cnt[indx+1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,width=1,label=&#39;noisy&#39;) ax.bar(cnt+2,category_cnt[indx+2],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;green&#39;,width=1,label=&#39;bad&#39;) else: ax.bar(cnt,category_cnt[indx],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=1) ax.bar(cnt+1,category_cnt[indx+1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,width=1) ax.bar(cnt+2,category_cnt[indx+2],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;green&#39;,width=1) xticks.append(cnt+.5) cnt = cnt+4 indx = indx+3 ax.set_xticks(xticks) ax.set_xticklabels(labels,rotation=30,fontsize=13) ax.grid(True) ax.set_xlim(-2,cnt+2) ax.legend(loc=&#39;upper right&#39;,frameon=False,bbox_to_anchor=(1.05,1),fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.yticks(fontsize=13) plt.show() . . In another post we will attempt to describe the acoustic features of the audio samples. Looking forward to have you with us in in this exploration. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/15/visualize_metadata.html",
            "relUrl": "/coswara/2020/08/15/visualize_metadata.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "What's inside the Coswara database",
            "content": "About the database . We have released a web application for data collection via crowsourcing. Essentially, this way anyone with a mobile phone and internet connectivity can contribute to the dataset. The website link can be accessed here. We target particpants from following three populations: . healthy: these are individuals with no respiratory illness | unhealthy: these are individuals with respiratory illness | COVID-19 positive: these are individuals identified as COVID-19 positive after RT-PCR test | . Metadata description . When a user opens the website, s/he is asked to fill a short questionaire which helps us collect metadata to categorize the user into one of the above three population. The complete metadata is composed of age, gender, location (country, state/province), current health status (healthy / exposed / cured / infected) and the presence of co-morbidity (pre-existing medical conditions) information. We do not collect any personally identifiable information. Each user gets a unique anonymized ID during data storage. A screenshot of the coswara webpage is provided below. Here, page 1 and 2 corresponnd to the metadata collection. . . Sound sample description . In the screenshot shown above, page 3 corresponds to the &quot;audio sample collection&quot; step. We collect audio samples corresponding to nine categories shown in the figure below. These categories as chosen to capture sound signals which embed in them most of the attributes of the respiratory system associated with speech production. To understand how let&#39;s tae a small detour to speech production system. . Art of speaking . The human speech production system draws contributions from diaphragm, lungs, trachea, larynx, pharynx, tounge, nasal cavity, and lips. You may note that many of these organs are not solely dedicated for speech production, example, we use mouth for eating and lungs to purify the air! Speech and vocal sound production is is an extra feat achieved, thanks to evolution, by these organs. . Lungs have elastic property, as they are in some sense repurposed swim bladders. Duing normal respiration, the diaphragm and the abdominal muscles between the ribs work together to expand the lungs. The elastic recoil of the lungs then provides the force that expels air during expiration. This means that the alveolar air pressure increases when you inhale and decreases when you exhale. Something different happens when you speak. . Speaking happens during exhaling. The aleveolar air pressure is released gradually in a co-ordinated manner via the opening and closing of the vocal cords (in the glottis). Something interesting happens here. For voiced sounds, such as vowels, the vocal folds open and close in a periodic fashion. This rate of opening and closing results in imparting periodicity to the output sound pressure wave. Further, this periodicity is one of the easiest perceived attributes in speech, and is referred to as the pitch of the speaker. You would have noticed that male speakers usually have lower pitch than female speakers, and female speakers have lower pitch than kid speakers. Why so? This is related to mass of the vocal folds. Heavier mass means lower pitch, and male anatomy often reveals a higher mass of the vocal folds. But that does not you cannot change your pitch. You can by altering the tension of the vocal folds, and we often do this when we want to emphasize something in our speech. Another attribute of speech we perceive quite easily is loudness. Increase in airflow from the lungs blows the vocal folds wider apart resulting in increased strength of the output pressure wave, thus making the sound louder. Voiced sounds are just one category of speech sounds. For unvoiced sounds, such as fricatives, the vocal folds remain open, and for stop consonant sounds, the vocal folds remain closed. Note that, these sounds do not have any perceived pitch associated with them. The below figure shows a schematic of human speech production system. . . What happens during coughing? The textbook explaination suggests that cough is a reflex action. The diaphragm contract, creating a negative pressure around the lung, and the glottis opens. This enables air to rush into the lungs in order to equalise the pressure. The glottis closes and the vocal cords contract to shut the glottis. The abdominal muscles contract to accentuate the action of the relaxing diaphragm, simultaneously, the other expiratory muscles contract. These actions increase the pressure of air within the lungs. The vocal cords relax and the glottis opens, releasing air at over 100 mph. The bronchi and non-cartilaginous portions of the trachea collapse to form slits through which the air is forced, which clears out any irritants attached to the respiratory lining. So a single cough will have no periodic opening and closing of vocal folds, unlike in vowels. However, often natural coughing results in a sequence of 3-4 coughs, and the physiological description of the opening and closing of glottis can become difficult to describe. An attempt to understand this is made here . What happens during breathing? The glottis largely remains open to enhance free flow of air into and out from the lungs, co-ordinated by the movement of the diaphragm and elasticity of the lungs. A nice video is shown here. . Why nine sound categories . As discussed above, we ask every user to record and upload nine sound samples. These can be grouped as follows: . breathing (two kinds; shallow and deep) | coughing (two kinds; shallow and heavy) | . The choice of the above two is is driven by the reporting by WHO and CDC which have listed dry cough, difficulty in breathing, and chest pain (or pressure) as key symptoms of this viral infection, visible between 2-14 days after exposure to the virus. Also, a recent modeling study of symptoms data collected from a pool of 7178 COVID-19 positive individuals validated the presence of these symptoms, and proposed a real-time prediction and tracking approach. Repeated coughing can adversely impact the mass and tension in the vocal folds. This can in turn alter the speaking style of the patient. You might of noticed that you can make a guess if your friend has cold his/her speaking style over phone. . sustained vowel phonations (three kinds; /ey/~as in made, /i/~as in beet, /u:/ as in cool) | . The choosen vowels have special place in the quantal theory of speech. These vowels are easy to produce and appear alomost in every spoken language. Further, these vowel sounds are perceived as most distinct amongst all other vowels, and have been argued to capture the vocal tract attributes effectively. For more details see here and here. . one to twenty digit counting (two kinds; normal and fast paced) | . Counting a sequence of digits corresponds to continuous speaking for close to 20 secs. Any breathing difficulty will make this task difficult, and we expect this to reflect in the speaking style such as loudness, stress and pause patterns, and pace of speaking. . Visualizing the waveforms . In the figure below we show an illustration of the waveforms and the corresponding spectrograms of few sound samples. The waveforms represent the recorded time-domain signal. Here, the spectrogram depicts the spectral content of the signal in every 10 msec short-time window of the signal. We can make some observations from the shown plots. . . The breathing samples are wideband. The spectral energy is distributed over all frequecies. The inhale is lower in energy than the exhale however, both lasts for a similar time-spane, close to 1 sec. The exhale (the center burst) also depicts some formant like structure in the spectrogram. This can be expected as the air travels through the vocal tract. . | For the coughing samples, we can see that these are repeating, and the first cough is little longer in duration. This can often happen as usually we take a deep breathe and release more in the first cough. Also, we can now see some formant structure also in the spectrogram. . | For sustained vowel phonations, we can see clear distinct formant structure, specifically, for the second formant in the spectrogram. . | For the digit counting, we can see the fluctuating formant structure in the spectrogram. . | . It should be noted that these recordings are obtained via crowdsourcing and recorded through web browsers. All sound samples are recorded at 48 kHz in WAV file format. Some of these recording may have ambient noise which cannot be filtered while recording. In another post we will try quantifying different artifacts we observe in these files. We manually listen to every uploaded file, and will share are opinion on the quality and curation procedure. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/11/metadata.html",
            "relUrl": "/coswara/2020/08/11/metadata.html",
            "date": " • Aug 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Speech signals - a wonder",
            "content": "Some context . &quot;Tring, tring&quot;,... your phone is ringing. You pick it, and hear - &quot;Hello, what&#39;s up?&quot;. Often, this sound, pouring into your ears, will inform you about the gender of the talker, the identity, and the emotion. Isn&#39;t this fascinating? With only 2-3 secs of sound signal, the human brain has estimated so much about the talker. As the converstation progresses, you will be able to even estimate the personality, and the health status of the talker. Speech signal is immensely rich in information, and our brain is trained, since our childhood, to extract a lot from it. . Speech machinery . Let&#39;s see how this fascinating sound signal -speech, is produced. You inhale a breath, the air enters your lungs and creates a high pressure. When you speak, a sequence of co-ordinated mechanical processes are initiated, flexing the vocal tract muscles, and the result is the release of air pressure from mouth and nose. If you don&#39;t believe this - just pause - take a deep breathe, and read aloud the previous line. Didn&#39;t you exhale while reading it! The co-ordinated mechanical processes involved during speaking are a miracle. Just to give a context, human beings are the only species on earth which can produce the diverse range of acoustic sounds making up our the vocal communication repository. Biology suggests that the reason is linked to the FOXP2 gene - found only in humans. Every human is unique, and this uniqueness also reflects in the speech signals which allows us to easily recognize the voice of many. . Recording speech . Satisfying the human curisoity to record speech signals was a challenge. How do you store the speech sounds? Speak into a jar, close the lid, and open it, and bingo, you hear it! Sorry, physics won&#39;t allow this happen. In late 19th century, phonograph was invented, a beutiful mind behind this was Thomas Edison. As time progressed, this became popular to record music, and lead to gramophone, magentic tape cassetes, and compact discs. Now we just store it in solid state semiconductor drives without worrying about how! Technology has been a blessing when you consider the seamless manner in which we capture, store, process, and playback speech, music, and images. . Processing speech . Cool! we are able to record sound signals. Let&#39;s move on to processing sound signals. Can we design machine systems which extract information from sound signals? For instance, can machines perform speech recognition, speaker recognition, emotion recognition, and ... the list can go on. Our brains does all this. It shouldn&#39;t be impossible to design machines to do this, and beyond this too! Well, automatic speech recognition is now a reality, and accessible in our mobile phones/laptops. Challenges exist when the speech recording is noisy, accented, not in English, or has multiple talkers etc. For single talker, clean recording, American/British accent, English speech, the machine systems work quite well. Similar is the performance for talker recognition as well. What is the key behind this technology? Welcome to the world of data analysis and machine learning! . Enters signal processing . In the follow-up posts we will discuss about to process and analyze speech signals using python. Looking forward to have you in this expedition! .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/06/20/speech_signals.html",
            "relUrl": "/coswara/2020/06/20/speech_signals.html",
            "date": " • Jun 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This blog shares our journey in the Coswara project. The project is launched by the LEAP lab, based at the Indian Institute of Science, Bangalore. .",
          "url": "https://iiscleap.github.io/coswara-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://iiscleap.github.io/coswara-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}