{
  
    
        "post0": {
            "title": "Coswara-Data Metadata Details",
            "content": "The COSWARA database is created by crowdsourcing sound samples. In this post we visualize the crowd (or participant) distribution along certain dimensions collected in the metadata questionnaire. The metadata and sound files are publicly available in the COSWARA-DATA github repository. In the below code we will read a CSV file containing the metadata of all participants (as of 23 Nov 2020), and plot the metadata distributions. . Step 1: . Load the CSVs . #collapse # import some packages #!/usr/bin/env python3 # -*- coding: utf-8 -*- &quot;&quot;&quot; Created on Sun Feb 10, 2020 @author: neeks, cmu &quot;&quot;&quot; import numpy as np from os import path import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import librosa from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) import matplotlib as mpl sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) # urls for csvs url_metadata_annotations = &#39;https://raw.githubusercontent.com/iiscleap/Coswara-Exp/master/Annotated_v2/Annotated_v2_ans.csv&#39; url_metadata_recordings = &#39;https://raw.githubusercontent.com/iiscleap/Coswara-Data/master/combined_data.csv&#39; path_audio_data = &#39;/Volumes/BackupNeeks/dBase/coswaradBase/annotatedFiles/audio_19Oct2020/&#39; path_store_figure = &#39;./figures/&#39; fig_save = 0 # load the csvs df_2 = pd.read_csv(url_metadata_recordings) # recording metadata . . Step 2: . Visuaize the gender distribution . #collapse DF = df_2.copy() # plot gender gender_labels = DF[&#39;g&#39;].unique()[::-1] gender_cnt = [] for i in range(len(gender_labels)): gender_cnt.append(len(DF[(DF[&#39;g&#39;] == gender_labels[i])]))# &amp; (DF[&#39;recording&#39;]==&#39;breathing-deep&#39;)])) clr_1 = &#39;tab:blue&#39; clr_2 = &#39;tab:red&#39; fig, ax = plt.subplots(figsize=(2, 6)) ax.bar(2,gender_cnt[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=clr_1,width=.6) ax.bar(4,gender_cnt[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot;//&quot;,color=clr_2,width=.6) for i, v in enumerate(gender_cnt): ax.text(2*(i+1)-.3,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) plt.xticks([2,4], [&#39;MALE&#39;,&#39;FEMALE&#39;],rotation=0) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xticks(fontsize=14) plt.yticks(fontsize=14) ax.set_xlim(1,5) # ax.set_ylim(200,1500) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # ax.figure.savefig(path_store_figure+&quot;IS2020_coswara_metadata_gender.pdf&quot;, bbox_inches=&#39;tight&#39;) plt.show() . . Step 3: . How is the age distribution? . #collapse # plot age age_labels = DF[&#39;a&#39;].unique() age_cnt_male = [] age_cnt_female = [] for i in range(len(age_labels)): if age_labels[i] == &#39;X&#39;: age_labels[i] = 0 age_cnt_male.append(len(DF[(DF[&#39;a&#39;] == age_labels[i]) &amp; (DF[&#39;g&#39;]==&#39;male&#39;)])) age_cnt_female.append(len(DF[(DF[&#39;a&#39;] == age_labels[i]) &amp; (DF[&#39;g&#39;]==&#39;female&#39;)])) age_cnt_male = DF[(DF[&#39;g&#39;]==&#39;male&#39;)][&#39;a&#39;].values age_cnt_female = DF[(DF[&#39;g&#39;]==&#39;female&#39;)][&#39;a&#39;].values age_labels = [&#39;0-18&#39;, &#39;18-30&#39;, &#39;30-40&#39;, &#39;40-50&#39;, &#39;50-60&#39;, &#39;60-70&#39;, &#39;70-80&#39;] age_grouped_male = [] age_grouped_female = [] for i in age_labels: age_grouped_male.append(len(age_cnt_male[(age_cnt_male &gt; (int(i.split(&#39;-&#39;)[0])-1)) &amp; (age_cnt_male &lt; int(i.split(&#39;-&#39;)[1]))])) age_grouped_female.append(len(age_cnt_female[(age_cnt_female &gt; (int(i.split(&#39;-&#39;)[0])-1)) &amp; (age_cnt_female &lt; int(i.split(&#39;-&#39;)[1]))])) clr_1 = &#39;tab:blue&#39; clr_2 = &#39;tab:red&#39; fig, ax = plt.subplots(figsize=(7, 6)) ax.bar(np.arange(0,len(age_labels)),age_grouped_male, align=&#39;center&#39;,alpha=1,hatch=&quot; &quot;,ecolor=&#39;black&#39;,capsize=5,color=clr_1,width=.3,label=&#39;MALE&#39;) ax.bar(np.arange(0,len(age_labels))+.3,age_grouped_female, align=&#39;center&#39;,alpha=1,hatch=&quot; &quot;,ecolor=&#39;black&#39;,capsize=5,color=clr_2,width=.3,label=&#39;FEMALE&#39;) ax.legend(frameon=False,loc=&#39;upper right&#39;,fontsize=14) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xlabel(&#39;AGE GROUP&#39;, fontsize=14) plt.xticks(np.arange(0,len(age_labels)), age_labels,rotation=0,fontsize=14) plt.yticks(fontsize=14) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # if fig_save: # ax.figure.savefig(path_store_figure+&quot;IS2020_coswara_metadata_age.pdf&quot;, bbox_inches=&#39;tight&#39;) plt.show() . . Step 4: How many are from India and Outside? . #collapse # plot country country_labels = DF[&#39;l_c&#39;].unique() country_cnt = [] for i in range(len(country_labels)): country_cnt.append(len(DF[(DF[&#39;l_c&#39;] == country_labels[i])])) country_cnt = np.array(country_cnt) indx = np.argsort(country_cnt)[::-1] country_cnt = country_cnt[indx] country_labels = country_labels[indx] two_categories = [country_cnt[0],np.sum(country_cnt[1:])] two_labels = [&#39;INDIA&#39;,&#39;OUTSIDE&#39;] fig, ax = plt.subplots(figsize=(2, 6)) ax.bar(2,two_categories[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=clr_1,width=.4) ax.bar(4,two_categories[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot;//&quot;,color=clr_2,width=.4) plt.xticks([2,4],two_labels,rotation=0,fontsize=14) for i, v in enumerate(two_categories): ax.text(2*(i+1)-.3,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xticks(fontsize=14) plt.yticks(fontsize=14) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # if fig_save: # ax.figure.savefig(path_store_figure+&quot;IS2020_metadata_country.pdf&quot;, bbox_inches=&#39;tight&#39;) plt.show() . . Step 5: Which are the top 6 states from India contributing to the data? . #collapse # plot state state_labels = DF[&#39;l_s&#39;].unique() state_cnt = [] for i in range(len(state_labels)): state_cnt.append(len(DF[(DF[&#39;l_s&#39;] == state_labels[i])])) state_cnt = np.array(state_cnt) indx = np.argsort(state_cnt)[::-1][0:6] state_cnt = state_cnt[indx] state_labels = state_labels[indx] fig, ax = plt.subplots(figsize=(6, 6)) ax.bar(np.arange(0,len(state_cnt)),state_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=clr_1,width=.25) ax.set_ylabel(&#39;COUNT&#39;,fontsize=14) # # ax.text(1.5,-9,&#39;MEAN&#39;,horizontalalignment=&#39;center&#39;) plt.xticks(np.arange(0,len(state_cnt)),state_labels,rotation=30,fontsize=14) plt.yticks(fontsize=14) for i, v in enumerate(state_cnt): ax.text(i-.15,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # if fig_save: # ax.figure.savefig(path_store_figure+&quot;IS2020_metadata_state.pdf&quot;, bbox_inches=&#39;tight&#39;) plt.show() . . Step 6: . How is the distribution across healthy (and no COVID-19), respiratory ailments (and no COVID-19), and COVID-19 categories? . #collapse ##### init path to list path_to_lists = &#39;/Users/neeks/Desktop/Documents/work/code/python_codes/notebookCodes/coswara/lists/&#39; ##### DF_more = DF.replace(&#39;None&#39;, np.nan) DF_more = DF.replace(np.nan, False) DF_more[&#39;healthy&#39;] = False DF_more[&#39;covid&#39;] = 0.0 DF_more[&#39;unhealthy_but_no_covid&#39;] = False for i in range(len(DF_more)): # healthy if (DF_more.at[i,&#39;covid_status&#39;]==&#39;healthy&#39;) &amp; (DF_more.at[i,&#39;asthma&#39;]==False) &amp; (DF_more.at[i,&#39;cld&#39;]==False) &amp; (DF_more.at[i,&#39;cold&#39;]==False) &amp; (DF_more.at[i,&#39;cough&#39;]==False) &amp; (DF_more.at[i,&#39;pneumonia&#39;]==False) &amp; (DF_more.at[i,&#39;fever&#39;]==False): DF_more.at[i,&#39;healthy&#39;] = True # covid if (DF_more.at[i,&#39;covid_status&#39;]==&#39;positive_asymp&#39;): DF_more.at[i,&#39;covid&#39;] = 1 if (DF_more.at[i,&#39;covid_status&#39;]==&#39;positive_mild&#39;): DF_more.at[i,&#39;covid&#39;] = 2 if (DF_more.at[i,&#39;covid_status&#39;]==&#39;positive_moderate&#39;): DF_more.at[i,&#39;covid&#39;] = 3 # unhealthy but not covid if (DF_more.at[i,&#39;covid_status&#39;]==&#39;resp_illness_not_identified&#39;) &amp; ((DF_more.at[i,&#39;asthma&#39;]==True) | (DF_more.at[i,&#39;cld&#39;]==True) | (DF_more.at[i,&#39;cold&#39;]==True) | (DF_more.at[i,&#39;cough&#39;]==True) | (DF_more.at[i,&#39;pneumonia&#39;]==True)): DF_more.at[i,&#39;unhealthy_but_no_covid&#39;] = True health_categories_cnt_all = [] # get healthy health_categories_cnt_all.append(len(DF_more[(DF_more[&#39;healthy&#39;]==True)])) # get covid health_categories_cnt_all.append(len(DF_more[(DF_more[&#39;covid&#39;]&gt;0)])) # get resp. ail. health_categories_cnt_all.append(len(DF_more[(DF_more[&#39;unhealthy_but_no_covid&#39;]==True)])) clr = [&#39;tab:blue&#39;,&#39;tab:red&#39;,&#39;tab:green&#39;] fig, ax = plt.subplots(figsize=(5, 6)) ax.bar(2, health_categories_cnt_all[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=clr[0],width=.3) ax.bar(4, health_categories_cnt_all[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot;//&quot;,color=clr[1],width=.3) ax.bar(6, health_categories_cnt_all[2], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=clr[2],width=.3) plt.xticks([2,4,6],[&#39;HEALTHY&#39;,&#39;COVID-19&#39;,&#39;RESP. AIL n (NOT COVID)&#39;],rotation=0,fontsize=14) for i, v in enumerate(health_categories_cnt_all): ax.text(2*(i+1)-.1,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xticks(fontsize=14) plt.yticks(fontsize=14) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.xlim(1,7) # if fig_save: # ax.figure.savefig(path_store_figure+&quot;IS2020_metadata_country.pdf&quot;, bbox_inches=&#39;tight&#39;) plt.show() . . In another plot we will describe the details of a subset of these files which have been annotated. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/11/23/visualize_coswara_data_metadata.html",
            "relUrl": "/coswara/2020/11/23/visualize_coswara_data_metadata.html",
            "date": " • Nov 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Progression of COVID-19 spread",
            "content": "Linear Prediction.... What&#39;s that? . Correlations in a time-series . Consider you have a time-series data, discrete sampled from an underlying continuous-time signal such that, the data samples are uniformly spaced apart in time. An example is shown in the below figure. . import numpy as np import librosa import matplotlib.pyplot as plt import pickle from PyEMD import EMD import numpy as np import os import pandas as pd from sklearn.preprocessing import normalize from scipy.signal import medfilt from scipy.signal import savgol_filter import datetime from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable import matplotlib as mpl mpl.rcParams[&#39;figure.dpi&#39;] = 100 . Let us try to understand if there is any correlation between sample at current time with the sample ay previous time step. We can visualize this correlation by plotting x[n] vs x[n+1]. The plot below depicts this, and it is clear that there exists a high correlation. We can remove this correlation using PCA but we don&#39;t want to do this. Instead we will exploit this correlation to obtain a linear model for the time-series data. Welcome to linear prediction modeling. . #collapse X = signal[indx] Y = signal[indx+1] reg = LinearRegression(fit_intercept = False).fit(X.reshape(-1,1), Y) reg.score(X.reshape(-1,1), Y) Y_est = reg.coef_*X fig = plt.subplots(figsize=[6,6]) ax = plt.subplot(1,1,1) ax.grid(True) ax.scatter(signal[indx],signal[indx+1],color=&#39;tab:green&#39;,edgecolor=&#39;black&#39;) ax.plot(X,Y_est,&#39;-&#39;,color=&#39;gray&#39;) plt.ylabel(&#39;x [n+1]&#39;, fontsize=14) plt.xlabel(&#39;x [n]&#39;, fontsize=14) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.text(-.04,.04,&#39;R-square = &#39;+str(round(reg.score(X.reshape(-1,1), Y),2)),fontsize=14) plt.show() . . Modeling a time-series . Given a time series, we can compute some simple statistics, or even obtain power spectral density. But how about being able to predict the next sample in the series? If the signal samples are uncorrelated then prediction is hopeless, but not otherwise. Interestingly, time-series data obtained in an experiment is not white. Infact, peaks in the Fourier spectrum of a time-series data indicates that there is correlation between samples. Lets establish a model for the signal based on the assumption that the current sample can be predicted from the past samples and current (and) past inputs. $$ x[n] = sum_{k=1}^P a_k x[n-k] + G sum_{k=0}^Q b_ke[n-k], b_0=1 $$ . where, a_k and b_k are scalars and referred to as the linear prediction coefficients. . Does the above equation makes sense? Lets simplify. If we assume all a_ks are zero then we have a simple moving average (MA) model running on the input e[n]. If we assume all b_ks are zero we have an auto-regressive (AR) model. In this notebook we will focus on the AR model. Intuitively, this model says that the output x[n] is expressed as a linear function of past P output samples and the current input sample e[n]. This looks like a simple model, right? For instance, we could have introduced non-linearity but we did not. The advantage of this simple model is that we can minimize a cost function and estimate the a_ks. The estimation can get complicated if the signal model has non-linearity. . | Lets bring some more intution to what is e[n]. If x[n] is output from a system then e[n] models the input to the system. Alternatively, we can also interpret e[n] as the error signal which could not be modeled by the prediction using the past P output samples. That is, . | . $$Ge[n] = x[n] - sum_{k=1}^P a_k x[n-k]$$ Thus, we expect e[n] to be high in magnitude at time instants were our prediction using the past output samples is poor, and low otherwise. . How do we estimate a_ks? First we need a cost function (E), and then we minimize this cost function over a_ks. The cost function of choice is mean square square (M.S.E). That is, | . $$E = sum_ne[n]^2 = sum_n big(x[n]- sum_{k=1}^P a_k x[n-k] big)^2$$ . Some terminologies: P is referred to as the order of the AR model. How do we choose P? There is an answer but before going there we need to understand some more things. . | Once we a_k we have the signal model! But how does this help in ways other than predicting the next sample in the time series? . | . Linear prediction model as a time-series analysis tool . We can write the model in z-transform domain. Don&#39;t know z-transform? No worries, you can skip this part. Will you like to know about z-transform? Yes, then do see this wiki. Continuing, below is our system model and its z-transform. begin{align} x[n] = sum_{k=1}^P a_k x[n-k] + Ge[n] X[z] = sum_{k=1}^P a_k z^{-k}X[z] + E[z] X[z] = H[z] times E[z] H[z] = dfrac{1}{1- sum_{k=1}^P a_k z^{-k}} end{align} . Note that we are able to decompose (or &quot;factorize&quot;) the spectrum of the time-series x[n] into a product of two different spectrums. That is, $X[z]=H[z]E[z]$. This is cool. Why? Suppose I ask you to factorize 96 into two numbers. begin{align} 96 &amp;= 16*6 &amp;= 24*4 &amp;= cdots end{align} There are many possibilities. Can we ask the same question for a spectrum? Yes, and linear prediction offers us one such factorization. Okay, but what is its worth? To answer this we should visualize the H[z] and E[z] using some example time-series data. . Geting to code . Consider we have a time-series data, x, composed of N samples. Recalling the linear prediction signal model for x we have, $$ x[n] = sum_{k=1}^P a_k x[n-k] + e[n] $$ . The goal of the code will be to estimate the prediction coefficients $ {a_1, dots,a_P }$ . begin{align} begin{bmatrix} x_{n} x_{n+1} vdots x_{n+N} end{bmatrix} = begin{bmatrix} x_{n-1} &amp; x_{n-2} &amp; dots &amp; x_{n-P} x_{n} &amp; x_{n-1} &amp; dots &amp; x_{n+1-P} vdots &amp; ddots &amp; x_{n+N-1} &amp; x_{n+N-2} &amp; dots &amp; x_{n+N-P} end{bmatrix} begin{bmatrix} a_{1} a_{2} vdots a_{P} end{bmatrix} end{align} #collapse def autocorrelate(x,n): &#39;&#39;&#39; Computes normalized autocorrelations &#39;&#39;&#39; A = np.array([sum(x[0:len(x)-i:]*x[i:len(x):])/(len(x)) for i in range(n+1)]) return A/float(A[0]) def levinsonDurbin(R): &#39;&#39;&#39; Implemments Levinson-Durbin&#39;s iterative approach to compute prediction coefficients Compute normalized autocorrelations &#39;&#39;&#39; nPoles = len(Rm)-1 En = np.zeros(nPoles+1) Rc = np.zeros(nPoles) En[0] = Rm[0] Ak = np.array([-Rm[1]/Rm[0]]) Rc[0] = Ak V = Rm[0]-pow(Rm[1],2)/Rm[0] En[1] = V n = 0 while n &lt; (nPoles-1): alfa = sum(np.append(1,Ak)*Rm[n+2:0:-1]) rho=- alfa/V Rc[n+1] = rho V = V+rho*alfa En[n+2] = V Ak = np.append(Ak+rho*Ak[::-1],rho) n+= 1 return np.append(np.concatenate(np.ones((1,)),np.array(Ak))) def paper_levinsonDurbin(R): P = len(R)-1 E = [] E.append(R[0]) k = [] a_prev = np.zeros((P,)) a = np.zeros((P,1) for i in range(1,P): alpha = R[i] for j in range(1,i-1): alpha+ = a_prev[j]*R[i-j] k.append(-alpha/E[i-1]) a[i] = k[i] for j in range(1,i-1): a[j] = a_prev[j] + k[i]*a_prev[i-j] E.append((1-k[i]**2)*E[i-1]) return np.append(np.concatenate(np.ones((1,)),a)) def paper_levinsonDurbin(R): &#39;&#39;&#39; Levinson Durbin method. Implementation is based on the pseudo-code in: &quot;&quot; J. Makhoul, Linear Prediction: a tutorial, IEEEE, 1975. &#39;&#39;&#39; P = len(R)-1 E = [] E.append(R[0]) k = [0] a = np.zeros((P+1,)) a[0] = 1 i = 1 while i&lt; (P+1): j = 1 alpha = 0 while j&lt;i: alpha = alpha + a_prev[j]*R[i-j] j = j+1 k.append((-R[i]-alpha)/E[i-1]) E.append((1-k[i]**2)*E[i-1]) a[i] = k[i] j = 1 while j &lt; i: a[j] = a_prev[j] + k[i]*a_prev[i-j] j = j+1 a_prev = a.copy() i = i+1 return a def do_lpc(signal,lpOrder=20,windowSize=512,overlap=0): &#39;&#39;&#39; LPC main function &#39;&#39;&#39; windowShift = windowSize-int(windowSize*overlap) start_point = 0 end_point = windowSize R = np.zeros(signal.shape) if overlap == 0 else None signal_memory = np.zeros((lpOrder,)) if overlap == 0 else None F = []; A = []; while end_point &lt; len(signal): signalFrame = signal[start_point:end_point] Rx = autocorrelate(signalFrame,lpOrder) Ak = levinsonDurbin(Rx) A.append(Ak) w,h = freqz(1,Ak.T,windowSize) F.append(np.log(np.abs(h))) s = np.concatenate((signal_memory,signalFrame)) if overlap == 0: residual = np.convolve(s,Ak) R[start_point:end_point]+= residual[lpOrder:-lpOrder] signal_memory = signalFrame[-lpOrder:] E = np.log(np.sum(R[start_point:end_point]**2)/windowSize) F[-1] = F[-1] + E start_point+= windowShift end_point = start_point+windowSize return np.array(A).T,R,np.array(F).T,w . . for i in range(1,0): print(i) . Load the signal using librosa. The code below also converts the signal to 8 KHz sampling rate. . #collapse signal,fs = librosa.load(&#39;./my_data/nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39;,sr=8000) overlap = 0 windowSize=int(32e-3*fs) windowShift=windowSize-int(windowSize*overlap) lpOrder=12 Ak,Residual,FResp,FSamples=do_lpc(signal,lpOrder,windowSize=windowSize,overlap=overlap) f = plt.figure() ax=f.gca() ax.imshow(FResp) ax.set_aspect(.1) plt.yticks([i for i in np.arange(0,len(FSamples),64)],[int(FSamples[i]*100)/100.0 for i in np.arange(0,len(FSamples),64)]) plt.xticks([i for i in np.arange(0,FResp.shape[1],64)],[i*windowShift/float(fs) for i in np.arange(0,FResp.shape[1],64)]) ax.invert_yaxis() plt.xlabel(&#39;Time (sec)&#39;) plt.ylabel(&#39;Frequency (KHz)&#39;) plt.show() f = plt.figure() ax=f.gca() ax.plot([i/float(fs) for i in range(len(signal))],signal) ax.plot([i/float(fs) for i in range(len(Residual))],Residual) plt.xlim([0,len(Residual)/float(fs)]) #plt.xlim([3.2,3.25]) plt.xlabel(&#39;Time (sec)&#39;) plt.xlim([.5,.55]) plt.ylim([-.1,.1]) . . Text(0, 0.5, &#39;Frequency (KHz)&#39;) .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/covid/2020/11/19/covid_cases_plot.html",
            "relUrl": "/coswara/tutorial/covid/2020/11/19/covid_cases_plot.html",
            "date": " • Nov 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Linear Prediction_in progress",
            "content": "Linear Prediction.... What&#39;s that? . Correlations in a time-series . Consider you have a time-series data, discrete sampled from an underlying continuous-time signal such that, the data samples are uniformly spaced apart in time. An example is shown in the below figure. . #collapse import numpy as np import matplotlib.pyplot as plt import librosa from scipy.signal import freqz from sklearn.linear_model import LinearRegression signal, fs = librosa.load(&#39;./my_data/nMIOAh7qRFf3pqbchclOLKbPDOm1_vowel-a.wav&#39;, sr=16000) times = np.arange(0,len(signal))/fs indx = np.where((times&gt;0) &amp; (times&lt;4))[0] fig = plt.subplots(figsize=[14,4]) ax = plt.subplot(1,1,1) ax.grid(True) ax.plot(times[indx], signal[indx]) plt.ylabel(&#39;amplitude [in A.U.]&#39;, fontsize=14) plt.xlabel(&#39;time [in sec]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) plt.text(.25,.2,&#39;signal&#39;,fontsize=14) indx = np.where((times&gt;.745) &amp; (times&lt;.75))[0] fig = plt.subplots(figsize=[14,4]) ax = plt.subplot(1,1,1) ax.grid(True) ax.plot(times[indx], signal[indx],&#39;--o&#39;, markersize=5, markerfacecolor=&#39;tab:green&#39;, markeredgewidth=1.5, markeredgecolor=&#39;black&#39;) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) plt.text(.7457,.02,&#39;zoomed in signal snippet&#39;,fontsize=14) plt.show() indx = np.where((times&gt;.8) &amp; (times&lt;.90))[0] fig = plt.subplots(figsize=[14,4]) ax = plt.subplot(1,1,1) ax.grid(True) ax.plot(times[indx], signal[indx]) # ,&#39;--o&#39;, markersize=5, markerfacecolor=&#39;tab:green&#39;, # markeredgewidth=1.5, markeredgecolor=&#39;black&#39;) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) # plt.text(.7457,.02,&#39;zoomed in signal snippet&#39;,fontsize=14) plt.show() . . Let us try to understand if there is any correlation between sample at current time with the sample ay previous time step. We can visualize this correlation by plotting x[n] vs x[n+1]. The plot below depicts this, and it is clear that there exists a high correlation. We can remove this correlation using PCA but we don&#39;t want to do this. Instead we will exploit this correlation to obtain a linear model for the time-series data. Welcome to linear prediction modeling. . #collapse X = signal[indx] Y = signal[indx+1] reg = LinearRegression(fit_intercept = False).fit(X.reshape(-1,1), Y) reg.score(X.reshape(-1,1), Y) Y_est = reg.coef_*X fig = plt.subplots(figsize=[6,6]) ax = plt.subplot(1,1,1) ax.grid(True) ax.scatter(signal[indx],signal[indx+1],color=&#39;tab:green&#39;,edgecolor=&#39;black&#39;) ax.plot(X,Y_est,&#39;-&#39;,color=&#39;gray&#39;) plt.ylabel(&#39;x [n+1]&#39;, fontsize=14) plt.xlabel(&#39;x [n]&#39;, fontsize=14) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.text(-.04,.04,&#39;R-square = &#39;+str(round(reg.score(X.reshape(-1,1), Y),2)),fontsize=14) plt.show() . . Modeling a time-series . Given a time series, we can compute some simple statistics, or even obtain power spectral density. But how about being able to predict the next sample in the series? If the signal samples are uncorrelated then prediction is hopeless, but not otherwise. Interestingly, time-series data obtained in an experiment is not white. Infact, peaks in the Fourier spectrum of a time-series data indicates that there is correlation between samples. Lets establish a model for the signal based on the assumption that the current sample can be predicted from the past samples and current (and) past inputs. $$ x[n] = sum_{k=1}^P a_k x[n-k] + G sum_{k=0}^Q b_ke[n-k], b_0=1 $$ . where, a_k and b_k are scalars and referred to as the linear prediction coefficients. . Does the above equation makes sense? Lets simplify. If we assume all a_ks are zero then we have a simple moving average (MA) model running on the input e[n]. If we assume all b_ks are zero we have an auto-regressive (AR) model. In this notebook we will focus on the AR model. Intuitively, this model says that the output x[n] is expressed as a linear function of past P output samples and the current input sample e[n]. This looks like a simple model, right? For instance, we could have introduced non-linearity but we did not. The advantage of this simple model is that we can minimize a cost function and estimate the a_ks. The estimation can get complicated if the signal model has non-linearity. . | Lets bring some more intution to what is e[n]. If x[n] is output from a system then e[n] models the input to the system. Alternatively, we can also interpret e[n] as the error signal which could not be modeled by the prediction using the past P output samples. That is, . | . $$Ge[n] = x[n] - sum_{k=1}^P a_k x[n-k]$$ Thus, we expect e[n] to be high in magnitude at time instants were our prediction using the past output samples is poor, and low otherwise. . How do we estimate a_ks? First we need a cost function (E), and then we minimize this cost function over a_ks. The cost function of choice is mean square square (M.S.E). That is, | . $$E = sum_ne[n]^2 = sum_n big(x[n]- sum_{k=1}^P a_k x[n-k] big)^2$$ . Some terminologies: P is referred to as the order of the AR model. How do we choose P? There is an answer but before going there we need to understand some more things. . | Once we a_k we have the signal model! But how does this help in ways other than predicting the next sample in the time series? . | . Linear prediction model as a time-series analysis tool . We can write the model in z-transform domain. Don&#39;t know z-transform? No worries, you can skip this part. Will you like to know about z-transform? Yes, then do see this wiki. Continuing, below is our system model and its z-transform. begin{align} x[n] = sum_{k=1}^P a_k x[n-k] + Ge[n] X[z] = sum_{k=1}^P a_k z^{-k}X[z] + E[z] X[z] = H[z] times E[z] H[z] = dfrac{1}{1- sum_{k=1}^P a_k z^{-k}} end{align} . Note that we are able to decompose (or &quot;factorize&quot;) the spectrum of the time-series x[n] into a product of two different spectrums. That is, $X[z]=H[z]E[z]$. This is cool. Why? Suppose I ask you to factorize 96 into two numbers. begin{align} 96 &amp;= 16*6 &amp;= 24*4 &amp;= cdots end{align} There are many possibilities. Can we ask the same question for a spectrum? Yes, and linear prediction offers us one such factorization. Okay, but what is its worth? To answer this we should visualize the H[z] and E[z] using some example time-series data. . Geting to code . Consider we have a time-series data, x, composed of N samples. Recalling the linear prediction signal model for x we have, $$ x[n] = sum_{k=1}^P a_k x[n-k] + e[n] $$ . The goal of the code will be to estimate the prediction coefficients $ {a_1, dots,a_P }$ . begin{align} begin{bmatrix} x_{n} x_{n+1} vdots x_{n+N} end{bmatrix} = begin{bmatrix} x_{n-1} &amp; x_{n-2} &amp; dots &amp; x_{n-P} x_{n} &amp; x_{n-1} &amp; dots &amp; x_{n+1-P} vdots &amp; ddots &amp; x_{n+N-1} &amp; x_{n+N-2} &amp; dots &amp; x_{n+N-P} end{bmatrix} begin{bmatrix} a_{1} a_{2} vdots a_{P} end{bmatrix} end{align} #collapse def autocorrelate(x,n): &#39;&#39;&#39; Computes normalized autocorrelations &#39;&#39;&#39; A = np.array([sum(x[0:len(x)-i:]*x[i:len(x):])/(len(x)) for i in range(n+1)]) return A/float(A[0]) def levinsonDurbin(R): &#39;&#39;&#39; Implemments Levinson-Durbin&#39;s iterative approach to compute prediction coefficients Compute normalized autocorrelations &#39;&#39;&#39; nPoles = len(Rm)-1 En = np.zeros(nPoles+1) Rc = np.zeros(nPoles) En[0] = Rm[0] Ak = np.array([-Rm[1]/Rm[0]]) Rc[0] = Ak V = Rm[0]-pow(Rm[1],2)/Rm[0] En[1] = V n = 0 while n &lt; (nPoles-1): alfa = sum(np.append(1,Ak)*Rm[n+2:0:-1]) rho=- alfa/V Rc[n+1] = rho V = V+rho*alfa En[n+2] = V Ak = np.append(Ak+rho*Ak[::-1],rho) n+= 1 return np.append(np.concatenate(np.ones((1,)),np.array(Ak))) def paper_levinsonDurbin(R): P = len(R)-1 E = [] E.append(R[0]) k = [] a_prev = np.zeros((P,)) a = np.zeros((P,1) for i in range(1,P): alpha = R[i] for j in range(1,i-1): alpha+ = a_prev[j]*R[i-j] k.append(-alpha/E[i-1]) a[i] = k[i] for j in range(1,i-1): a[j] = a_prev[j] + k[i]*a_prev[i-j] E.append((1-k[i]**2)*E[i-1]) return np.append(np.concatenate(np.ones((1,)),a)) def paper_levinsonDurbin(R): &#39;&#39;&#39; Levinson Durbin method. Implementation is based on the pseudo-code in: &quot;&quot; J. Makhoul, Linear Prediction: a tutorial, IEEEE, 1975. &#39;&#39;&#39; P = len(R)-1 E = [] E.append(R[0]) k = [0] a = np.zeros((P+1,)) a[0] = 1 i = 1 while i&lt; (P+1): j = 1 alpha = 0 while j&lt;i: alpha = alpha + a_prev[j]*R[i-j] j = j+1 k.append((-R[i]-alpha)/E[i-1]) E.append((1-k[i]**2)*E[i-1]) a[i] = k[i] j = 1 while j &lt; i: a[j] = a_prev[j] + k[i]*a_prev[i-j] j = j+1 a_prev = a.copy() i = i+1 return a def do_lpc(signal,lpOrder=20,windowSize=512,overlap=0): &#39;&#39;&#39; LPC main function &#39;&#39;&#39; windowShift = windowSize-int(windowSize*overlap) start_point = 0 end_point = windowSize R = np.zeros(signal.shape) if overlap == 0 else None signal_memory = np.zeros((lpOrder,)) if overlap == 0 else None F = []; A = []; while end_point &lt; len(signal): signalFrame = signal[start_point:end_point] Rx = autocorrelate(signalFrame,lpOrder) Ak = levinsonDurbin(Rx) A.append(Ak) w,h = freqz(1,Ak.T,windowSize) F.append(np.log(np.abs(h))) s = np.concatenate((signal_memory,signalFrame)) if overlap == 0: residual = np.convolve(s,Ak) R[start_point:end_point]+= residual[lpOrder:-lpOrder] signal_memory = signalFrame[-lpOrder:] E = np.log(np.sum(R[start_point:end_point]**2)/windowSize) F[-1] = F[-1] + E start_point+= windowShift end_point = start_point+windowSize return np.array(A).T,R,np.array(F).T,w . . for i in range(1,0): print(i) . Load the signal using librosa. The code below also converts the signal to 8 KHz sampling rate. . #collapse signal,fs = librosa.load(&#39;./my_data/nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39;,sr=8000) overlap = 0 windowSize=int(32e-3*fs) windowShift=windowSize-int(windowSize*overlap) lpOrder=12 Ak,Residual,FResp,FSamples=do_lpc(signal,lpOrder,windowSize=windowSize,overlap=overlap) f = plt.figure() ax=f.gca() ax.imshow(FResp) ax.set_aspect(.1) plt.yticks([i for i in np.arange(0,len(FSamples),64)],[int(FSamples[i]*100)/100.0 for i in np.arange(0,len(FSamples),64)]) plt.xticks([i for i in np.arange(0,FResp.shape[1],64)],[i*windowShift/float(fs) for i in np.arange(0,FResp.shape[1],64)]) ax.invert_yaxis() plt.xlabel(&#39;Time (sec)&#39;) plt.ylabel(&#39;Frequency (KHz)&#39;) plt.show() f = plt.figure() ax=f.gca() ax.plot([i/float(fs) for i in range(len(signal))],signal) ax.plot([i/float(fs) for i in range(len(Residual))],Residual) plt.xlim([0,len(Residual)/float(fs)]) #plt.xlim([3.2,3.25]) plt.xlabel(&#39;Time (sec)&#39;) plt.xlim([.5,.55]) plt.ylim([-.1,.1]) . . Text(0, 0.5, &#39;Frequency (KHz)&#39;) .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/2020/09/12/linear-prediction.html",
            "relUrl": "/coswara/tutorial/2020/09/12/linear-prediction.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "What, how, and why of MFCCs",
            "content": "MFCC stands for mel-frequency cepstral coefficient. In this tutorial we will understand the significance of each word in the acronym, and how these terms are put together to create a signal processing pipeline for acoustic feature extraction. The resulting features, MFCCs, are quite popular for speech and audio R&amp;D. Why so? We will have an answer for this by the end of this notebook. . Say hi to our signal . It is good to understand by doing and hence, we will carry the below speech signal alongside us. Do listen to eat before we proceed. . #collapse import numpy as np import librosa from IPython.lib.display import Audio import matplotlib.pyplot as plt from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable import seaborn as sns fs = 16000 fname = &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_normal_count.wav&#39; dname = &#39;./my_data/&#39; # load x, sr = librosa.load(dname+fname,sr=fs) x = x/max(np.abs(x)) times = np.arange(0,len(x))/sr # listen Audio(x, rate=sr, autoplay=False) . . Your browser does not support the audio element. Below is the time-domain representation. The duration of this signal is 3.64 seconds, and it has 58378 samples (or data points). Our goal is to represent these 58378 data points with fewer numbers (or a compact representation) and still preserve the essential features of the signal. . #collapse # plot print(&#39;Nos. samples: &#39;+str(len(x))) print(&#39;Duration: &#39;+str(times[-1])+ &#39;seconds&#39;) fig = plt.subplots(figsize=(12,2)) ax = plt.subplot(1,1,1) ax.plot(times, x) ax.grid(True) plt.ylabel(&#39;amplitude [in A.U.]&#39;, fontsize=14) plt.xlabel(&#39;time [in sec]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . Nos. samples: 58378 Duration: 3.6485625seconds . Signal non-stationarity . If you plot a speech signal waveform you will note that the waveform shape changes significantly over time. This time is not in months or years but in tens of milliseconds. You don&#39;t believe, right? See below image. It depicts a speech signal. From the signal we take a 25 msec segment, compute its magnitude Fourier transform, and push the output into a column of an empty matrix. Then we hop by 10 msec along the speech signal (from left to right), and again take a 25 msec segment, and repeat the procedure - magnitude fourier transform, push into the column of the matrix, and hop, till we reach the end of the speech signal. The resulting matrix is referred to as the spectrogram. We have plotted this matrix as an image (color bar:color gradient from dark blue to white indicates high to low amplitude in the spectral content). Spend some time staring at the spectrogram and you would notice the changing spectrum across time! In a more technical sense, this changing spectral content over time behavior of sound signals is referred to as signal non-stationarity. If you think philosophically, a reason we enjoy speech and music signals is due the non-stationarity in these signals. A sinusoid (like, a tone signal) will bore us very quickly. . The worrying thing with signal non-stationarity is that the majority of statistics and signal processing methods are applicable to stationary signals. For instance, the methods assume that the underlying data distribution does not change over time (Ergodic process) or the Fourier spectrum stays the same. To apply these methods to speech and audio signals we will make use of short-time analysis. We will segment the audio signal into short-time segments (of duration windDur), and from each segment we will extract some compact representation. An illustration is shown in the below figure. Here, a 3-D representation is extracted from every short-time segment. . Windowing . When we take out a short-time segment from the full signal, the start and end of the short-time segment signal may have abrupt discontinuity. This will introduce spurious frequencies in the Fourier transform of the signal (more here). Hence, it is a usual practice to multiply the short-time segment with a window which tapers at start and end. Now because the window tappers at the ends, we will make the consecutive short-time segments overlap. This is controlled by hopDur. Below is the code to obtain short-time segments of the signal. . #collapse def segment_signal(signal, winType=&#39;rect&#39;, winDur = 25e-3, hopDur=10e-3, sr=16e3): # hop_size in ms winLen = int(winDur*sr) hopLen = int(hopDur*sr) signal = np.pad(signal, winLen//2, mode=&#39;reflect&#39;) nframes = int((len(signal) - winLen) / hopLen) + 1 frames = np.zeros((nframes,winLen)) if winType == &#39;hamming&#39;: window = np.hamming(winLen) window = window-np.min(window) window = window/np.max(window) elif winType == &#39;rect&#39;: window = np.ones((winLen,),dtype=float) for n in range(nframes): frames[n] = window*signal[n*hopLen:n*hopLen+winLen] return frames def nearestpow2(n): k=1 while n&gt;2**k: k = k+1 return 2**k hopDur = 10e-3 #ms winDur = 25e-3 x_segs_rect = segment_signal(x, winType=&#39;rect&#39;, winDur=winDur, hopDur= hopDur, sr=sr) x_segs_hamming = segment_signal(x, winType=&#39;hamming&#39;, winDur=winDur, hopDur= hopDur, sr=sr) fix, ax = plt.subplots(2,2,figsize=(14,8)) indx = 155 nfft = nearestpow2(x_segs_rect.shape[1]) axis_freq = np.arange(0,nfft/2+1)*sr/nfft axis_time = indx*hopDur+np.arange(0,x_segs_rect.shape[1],1)/sr X_1 = 10*np.log10(np.abs(np.fft.rfft(x_segs_rect[indx,:],nfft))) X_1 = X_1 - np.max(X_1) X_2 = 10*np.log10(np.abs(np.fft.rfft(x_segs_hamming[indx,:],nfft))) X_2 = X_2 - np.max(X_2) ax[0][0].plot(axis_time,x_segs_rect[indx,:]) ax[0][0].grid(True) ax[0][0].set_ylabel(&#39;AMPLITUDE [in A.U]&#39;, fontsize=14) ax[0][0].set_xlabel(&#39;time [in secs]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax[0][0].spines[&#39;right&#39;].set_visible(False) ax[0][0].spines[&#39;top&#39;].set_visible(False) ax[0][0].text(1.55,0.8,&#39;After rectangular windowing&#39;) ax[0][1].plot(axis_time,x_segs_hamming[indx,:]) ax[0][1].grid(True) ax[0][1].set_ylabel(&#39;AMPLITUDE [in A.U]&#39;, fontsize=14) ax[0][1].set_xlabel(&#39;time [in secs]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax[0][1].spines[&#39;right&#39;].set_visible(False) ax[0][1].spines[&#39;top&#39;].set_visible(False) ax[0][1].text(1.55,0.8,&#39;After hamming windowing&#39;) ax[1][0].plot(axis_freq,X_1) ax[1][0].grid(True) ax[1][0].set_ylabel(&#39;MAGNITUDE SPECTRUM [in dB]&#39;, fontsize=14) ax[1][0].set_xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax[1][0].spines[&#39;right&#39;].set_visible(False) ax[1][0].spines[&#39;top&#39;].set_visible(False) ax[1][0].text(1.55,0.8,&#39;After rectangular windowing&#39;) ax[1][1].plot(axis_freq,X_2) ax[1][1].grid(True) ax[1][1].set_ylabel(&#39;MAGNITUDE SPECTRUM [in dB]&#39;, fontsize=14) ax[1][1].set_xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax[1][1].spines[&#39;right&#39;].set_visible(False) ax[1][1].spines[&#39;top&#39;].set_visible(False) ax[1][1].text(1.55,0.8,&#39;After hamming windowing&#39;) ax[1][0].set_ylim(-40,5) ax[1][1].set_ylim(-40,5) plt.show() . . The consecutive short-time segments may have significant correlations. Lets visualize this by plotting the covariance matrix of x_segs_hamming. Below we visualize only a part of the correlation matrix. The brighter colors indicate high correlation (diagonal values are 1). We can see some blocks of high correlation. This is likely because the speech signal has burst of vocal activity (spokenn digits), with pauses in between. . #collapse x_segs = x_segs_hamming.copy() C = np.zeros((x_segs.shape[0],x_segs.shape[0])) # C = np.dot(x_segs.T,x_segs) C.shape for i in range(x_segs.shape[0]): for j in range(x_segs.shape[0]): C[i,j] = np.dot(x_segs[i,:],x_segs[j,:])/np.linalg.norm(x_segs[i,:])/np.linalg.norm(x_segs[j,:]) fig = plt.subplots(figsize=(12,6)) ax = plt.subplot(1,1,1) # ax.plot(C[110,:]) ax.imshow(np.abs(C[100:200,100:200])) plt.show() . . We hear spectral energies . Compute Fourier transform . Why? Psychoacoustic and physiology studies of the auditory system of human and animals have shown that we are sensitive to individual frequencies contained in the sound pressure variation. Hence, it makes sense to transform every short-time segment into corresponding Fourier spectrum representation. This is straight forward using the FFT implementation available in python. One thing to focus on here is the nfft length. This should be at least equal to winLen to get a meaningful FFT of each segment. . Compute spectral energies . The output of FFT is a complex vector. Psychoacoustic studies have shown that our hearing is sensitive to energy distribution across the frequencies. So, we compute the energy by taking the absolute values. Below is the code for the above two things . #collapse X = np.zeros((x_segs.shape[0],int(nfft/2)+1)) X = np.abs(np.fft.rfft(x_segs,nfft,axis=1))**2 . . Further, we hear in mel-scale . Mel-scale transformation of frequencies . There is a phenomenal psychoacoustic work which has attempted to quantify the mapping between the frequency scale obtained from Fourier transform and that perceived by our brain. . Volkmann and Stevens worked on constructing a scale that reflected how people hear musical tones. Listeners were asked to adjust tones so that one tone (a sinusoidal signal of specific frequency) was half as higher as another, and other such subdivisions of the frequency range. This way the mel scale (mel stands for melody) was obtained. You can read more in this article. A standard definition of mel-scale is: a perceptual scale of pitches judged by listeners to be equal in distance from one another. The reference of 1000 mels was assigned as having a frequency of 1000 Hz (at 40 dB above threshold). The below code visualizes this mapping. We can make the following quick observations: . the mapping is non-linear | monotonically increasing in shape | close to linear till 1000 Hz | beyond 1000 Hz it is highly compressive This implies doubling in the linear scale does not result in doubling in the mel-scale! | . #collapse def freq_to_mel(freq): # converting linear scale frequency to mel-scale return 2595.0 * np.log10(1.0 + freq / 700.0) def mel_to_freq(mels): # converting mel-scale frequency to linear scale return 700.0 * (10.0**(mels / 2595.0) - 1.0) axis_freqs = np.arange(0,nfft/2+1)/nfft*sr mels = freq_to_mel(axis_freqs) fig = plt.subplots(figsize=[8,5]) ax = plt.subplot(1,1,1) ax.plot(axis_freqs,mels, color=&#39;tab:red&#39;) ax.plot(axis_freqs[:100],axis_freqs[:100],&#39;--&#39;,color=&#39;gray&#39;) ax.grid(True) ax.set_ylabel(&#39;FREQUENCY [in mels]&#39;, fontsize=14) ax.set_xlabel(&#39;FREQUENCY [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.text(3000,1500,&#39;mel-scale warping&#39;,fontsize=14) plt.show() . . Designing a spectral energy weighting filterbank . Remember, our goal is to achieve a compact representation of the speech signal. We have some M (equal to 365) frames, each of dimension 257. Is there a way to reduce the dimension 257 to N, something smaller? . Yes, we can make N bins along 1 to 257, and sum the spectral energy in each of these bins. We will get N numbers and thus would have reduced the 257 dimension to N. Now the key questions are: . What should be the size of these bins? | Should these be of uniform sizes from 1 to 257? | . The frequency warping from linear to mel-scale suggests that non-uniform size bins should be preferred, with the size of the bins gradually increasing. So, we make uniform bins in the mel frequency scale, and this will result in corresponding non-uniform size bins in the linear frequency scale. The figure below shows an illustration for this. . Once we have the binning. Next question is what kind of weighting should we apply to the energies in these bins. A preferred choice is triangular weighting. This will then also imply that we should make the consecutive bins have an overlap 50%. The below code implements these steps and creates the spectral energy weighting filterbank. This is also widely referred to as the mel-scale uniform bandwidth filterbank. Also, it is good to normalize the area under each filter weighting function. This makes the relative peak strength of the filters decrease as we go towards higher center frequency filters. . #collapse freq_min = 0 freq_high = sr / 2 mel_filter_num = 40 def get_filter_points(fmin, fmax, mel_filter_num, nfft, sample_rate=16000): fmin_mel = freq_to_mel(fmin) fmax_mel = freq_to_mel(fmax) mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num+2) freqs = mel_to_freq(mels) return np.floor((nfft) / sample_rate * freqs).astype(int), freqs def get_filters(filter_points, nfft): filters = np.zeros((len(filter_points)-2,int(nfft/2+1))) for n in range(len(filter_points)-2): filters[n, filter_points[n] : filter_points[n + 1]] = np.linspace(0, 1, filter_points[n + 1] - filter_points[n]) filters[n, filter_points[n + 1] : filter_points[n + 2]] = np.linspace(1, 0, filter_points[n + 2] - filter_points[n + 1]) return filters filter_points, freqs = get_filter_points(freq_min, freq_high, mel_filter_num, nfft=nfft, sample_rate=sr) filters = get_filters(filter_points, nfft=nfft) fig = plt.subplots(figsize=[16,5]) ax = plt.subplot(1,2,1) for n in range(filters.shape[0]): ax.plot(axis_freqs,filters[n]) ax.grid(True) ax.set_ylabel(&#39;WEIGHTING [in A.U]&#39;, fontsize=14) ax.set_xlabel(&#39;FREQUENCY [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.text(3000,1.1,&#39;uninform mel-bandwidth filterbank&#39;,fontsize=14) # normalizing the filter weighting based on area enorm = 2.0 / (freqs[2:mel_filter_num+2] - freqs[:mel_filter_num]) filters *= enorm[:, np.newaxis] ax = plt.subplot(1,2,2) for n in range(filters.shape[0]): ax.plot(axis_freqs,filters[n]) ax.grid(True) ax.set_ylabel(&#39;WEIGHTING [in A.U]&#39;, fontsize=14) ax.set_xlabel(&#39;FREQUENCY [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.text(3000,.0055,&#39;filter area normalized&#39;,fontsize=14) plt.show() . . Applying the filterbank . Once we have the filter bank we apply this to the magnitude FFT X to transform 257x357 to Nx357. Here, N will be equal to the number of mel filters (mel_filter_num) in the filterbank. Also, we apply a log to the output values. This is because you would have noticed that you can hear faint and loud sounds. Applying log transformation will help us amplify the low energy values and attenuate little bit the high amplitude values (to know more, you can read about it more here - Weber-Fechner Law). . X_filtered = np.dot(filters, X.T) X_filtered_log = 10.0 * np.log10(X_filtered) fig = plt.subplots(figsize=[16,5]) ax = plt.subplot(1,2,1) ax.imshow(10*np.log10(X.T),origin=&#39;lower&#39;,aspect=&#39;auto&#39;,cmap=&#39;RdBu_r&#39;) ax.grid(True) ax.set_ylabel(&#39;linear frequency FFT bin&#39;, fontsize=14) ax.set_xlabel(&#39;short-time frame index&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax = plt.subplot(1,2,2) ax.imshow(X_filtered_log,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,cmap=&#39;RdBu_r&#39;) ax.grid(True) ax.set_ylabel(&#39;filter index&#39;, fontsize=14) ax.set_xlabel(&#39;short-time frame index&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . Decorrelation . Decorrelating filterbank outputs . The following are some data analysis convenience steps, and (likely) not what happens inside the ear or brain. These steps will help us to further reduce the dimension from Nx357 to Kx357 where K &lt; N. Remember, our goal is to obtain a compact representation. . Lets try to see if there is correlation across the filter outputs. If there is, we can reduce the dimension further. How to go ahead? We will do a PCA on X_scaled and visualize the decrease in explained variance across components. You can see in the figure below it does drop sharply, and likely 5 components suffice to capture most of the data variance. . #collapse pca = PCA(n_components=X_scaled.shape[0]) pca.fit(X_scaled.T) fig = plt.subplots(figsize=[8,5]) ax = plt.subplot(1,1,1) ax.plot(pca.explained_variance_ratio_,&#39;-o&#39;) ax.grid(True) ax.set_ylabel(&#39;explained variance&#39;, fontsize=14) ax.set_xlabel(&#39;PCA component&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . Lets also visualize how the principal components look. These look a little oscillatory. . fig = plt.subplots(figsize=[12,10]) for i in range(5): ax = plt.subplot(5,1,i+1) ax.plot(pca.components_[i,:]) ax.set_ylabel(&#39;PCA- &#39;+str(i+1), fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) . Drawn by this observation and make our next linear transformation data independent, we can use the discrete cosine transform. . #collapse def dct(dct_filter_num, filter_len): basis = np.empty((dct_filter_num,filter_len)) basis[0, :] = 1.0 / np.sqrt(filter_len) samples = np.arange(1, 2 * filter_len, 2) * np.pi / (2.0 * filter_len) for i in range(1, dct_filter_num): basis[i, :] = np.cos(i * samples) * np.sqrt(2.0 / filter_len) return basis dct_basis = dct(40,40) fig = plt.subplots(figsize=[16,4]) ax = plt.subplot(1,1,1) for i in range(10): ax.plot(dct_basis[i,:]) plt.ylabel(&#39;DCT basis amplitudes&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) . . We apply the DCT transform to the X_filtered_log and the result thus obtained is called MFCCs. The MFCC[0], the first element in the vector obtained after DCT captures the spectral energy across the filterbank, for each short-time frame. This can be seen in the plot below. Interpreting the other dimensions of MFCCs is not straight forward. Also, the visualization does not mean much. But this compact representation is very useful as feature vectors for classification algorithms. . #collapse dct_filter_num = X_scaled.shape[0] dct_filters = dct(dct_filter_num, X_filtered_log.shape[0]) cepstral_coefficents = np.dot(dct_filters, X_filtered_log) fig = plt.subplots(figsize=[16,5]) ax = plt.subplot(1,2,1) ax.imshow(X_filtered_log,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,cmap=&#39;RdBu_r&#39;) ax.grid(True) ax.set_ylabel(&#39;filter index&#39;, fontsize=14) ax.set_xlabel(&#39;short-time frame index&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax = plt.subplot(1,2,2) ax.imshow(cepstral_coefficents,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,cmap=&#39;RdBu_r&#39;) ax.grid(True) ax.set_ylabel(&#39;MFCCs&#39;, fontsize=14) ax.set_xlabel(&#39;short-time frame index&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() fig = plt.subplots(figsize=[16,5]) ax = plt.subplot(1,2,1) ax.plot(cepstral_coefficents[0,:]) ax.grid(True) ax.set_ylabel(&#39;MFCC[0]&#39;, fontsize=14) ax.set_xlabel(&#39;short-time frame index&#39;, fontsize=14) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax = plt.subplot(1,2,2) ax.imshow(cepstral_coefficents[1:,:],origin=&#39;lower&#39;,aspect=&#39;auto&#39;,cmap=&#39;RdBu_r&#39;) ax.grid(True) ax.set_ylabel(&#39;MFCCs[1:39]&#39;, fontsize=14) ax.set_xlabel(&#39;short-time frame index&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . . Summary . We obtained a compact representation of the speech signal we started with. . Specifically, the 3.64 seconds signal with 58378 samples is now represented by 40x357 = 14280 samples. This is close to 1/4 of the original sample size. We can reduce it further by just picking only the first 11 MFCCs (as we noticed that the explained variance falls very rapidly, and 11 is also a common choice in the speech processing application domain). We then have a further compact representation of size, 11x357 = 3927, and this is a splendid compression by a factor close to 15x. | Further, our compact representation is based on some insights from psychoacoustic studies. | . What makes MFCCs popular: . Its implementation is quite easy in a data processing pipeline. It makes use of simple and efficient operations such as FFT, logarithms, and DCT. | The mel-scale is drawn from psychoacoustics studies. It is always inspiring to make use of some operation which is also hypothesized (and proven at least for tones) to be used by our perception. To give us a feedback please us the comment box below. . | . References . If you are interested to know more, the below references will be useful. . The excellent discussion section in L. C. W. Pols, H. R. C. Tromp, and R. Plomp Frequency analysis of Dutch vowels from 50 male speakers, 1972. | Stan W. Davis, P. Mermelstein, Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences, 1980. . | This post at Kaggle served both as a motivation and improve the understanding to write the notebook. The mel filterbank design code segment is taken from this notebook. . | You can find some more posts also here and here. . | .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/2020/08/20/mfcc.html",
            "relUrl": "/coswara/tutorial/2020/08/20/mfcc.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Acoustic Feature Extraction",
            "content": "With the advent of well engineered sensors, the practice of data collection has gained pace. Individuals and groups have put significant efforts to record and host hours of audio recordings on the internet. Also, many of these are accessible at the click of a mouse. Is all data also information? Information is always for a purpose, and hence, the proper question will be - is this data informative for so and so task? To answer this we have to go beyond just visualizing sound signals, and understand the concept of features. . Example:How do you distinguish tomatoes from potatoes, without tasting? You will likely use characteristics such as color (red vs brown), shape (close to spherical vs somewhat spherical), tightness (soft vs hard), etc. What we listed are the features of vegetables, and values these features take can help us distinguish one vegetable from another. How? In the figure shown below we show a cartoon schematic of intensity of reflected light by a 100 potatoes and tomatoes. We can see that a majority of tomatoes reflect more light than potato and hence, this can serve as a feature to distinguish one from the other. We also see that around an intensity of 102 the distinction is not very clear. This means some of the potatoes may get confused with tomatoes, and vice versa. This implies, the selected feature cannot distinguish all kinds of potatoes from tomatoes. In general, a combination of features can give better distinctions amongst the categories. . #collapse import numpy as np import matplotlib.pyplot as plt import seaborn as sns np.random.seed(seed=10) x = [] x.append(np.random.randn(100)+100) x.append(2*np.random.randn(100)+105) # make plot fig = plt.subplots(figsize=(8,4)) ax = plt.subplot(1,1,1) sns.distplot(x[0],color=&#39;r&#39;,label=&#39;potato&#39;) sns.distplot(x[1],color=&#39;b&#39;,label=&#39;tomato&#39;) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.legend(frameon=False) plt.xlabel(&#39;(May be) intensity of reflected light [in A.U.]&#39;) plt.ylabel(&#39;DENSITY&#39;) plt.show() . . &lt;Figure size 800x400 with 1 Axes&gt; . Features of sound signals . What features can help distinguish one kind of sound from another? We can treat a sound signal (time-series) as an object (like we did for tomatoes and potatoes) and consider features such as average amplitude, variance in amplitude values, duration of silence, etc. A cartoon illustration is shown in the below figure. . . To be honest, such a simple file level feature won&#39;t help in distinguishing different sound types. Can we be a little more clever? Yes, and we can ask the following questions: . Do we know how our ears process the sound signals? We know a lot about this from the field of physiology. Infact, contributions towards answering this have also been awarded a Nobel Prize (Georg von Békésy, 1961). It seems our hearing system housed inside the ear does a spectral analysis of the pressure variations. This spectral information is encoded in neural firings and sent to the higher auditory pathways, including the brain. Instead of writing a lot I will suggest you see this video here. | Do we know how our brain distinguishes a dog barking from a tiger roaring? The answer to this is being actively studied in the field of psychoacoustic (how does mind process sound). We know from several studies that the spectral information is critical for perception. We know a lot more about speech signals. For example, we know that pitch, loudness, and timbre are critical in distinguishing one voice from another. | . Based on these observations researchers have designed methods to estimate numerous features from sound signals. Two python packages we will use are Librosa and Parselmouth. A key aspect which is quite widely popular in sound signal analysis is to estimate features from short-time segments of the signal (instead of the file level features depicted in the previous figure). These short-time segments are usually 25 msec in duration (Why?). The below figure provides an illustration. . Let&#39;s write some code to extract the intensity of sound computed over short-time segments from a cough sound signal. From the plot shown below you can see that intensity is a single number, and it is high at instants of coughs. . #collapse import parselmouth import librosa from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable sr = 16000 fname = &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39; dname = &#39;./my_data/&#39; # for PRAAT hop_dur = .01 num_form = 3 max_form_freq = 5500 STANDARD_INTENSITY = 70. # call parselmouth to load sound snd = parselmouth.Sound(dname+fname) snd.scale_intensity(STANDARD_INTENSITY) x = snd.values[0] sr = snd.sampling_frequency times = np.arange(0,len(x),1)/sr # estimate intensity and spectrogram intensity = snd.to_intensity(minimum_pitch = 200.0, time_step=hop_dur,subtract_mean=False).values[0] spectrogram = snd.to_spectrogram(window_length=0.04) fig = plt.subplots(figsize=(12,12)) ax = plt.subplot(3,1,1) ax.plot(times,x) ax.set_xlabel(&quot;TIME [in s]&quot;) ax.set_ylabel(&#39;AMPLITUDE[A.U.]&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.xlim(times[0],times[-1]) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(3,1,2) ax.plot(np.arange(0,len(intensity),1)*hop_dur,intensity) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;intensity [in dB]&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.xlim(times[0],times[-1]) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fig,ax = plt.subplots(1,1,figsize=(12,5)) ax = plt.subplot(3,1,3) dynamic_range = 70 X, Y = spectrogram.x_grid(), spectrogram.y_grid() sg_db = 10 * np.log10(spectrogram.values) im = ax.pcolormesh(X, Y, sg_db, vmin=sg_db.max() - dynamic_range, cmap=&#39;RdBu_r&#39;) plt.ylim([spectrogram.ymin, spectrogram.ymax]) plt.xlabel(&quot;TIME [in s]&quot;) plt.ylabel(&quot;FREQUENCY [in Hz]&quot;) plt.show() . . Acoustic Features from Coswara Database . Lets visualize some acoustic features of sound samples from the COSWARA database. We will visualize different features via grouping of sound identity and gender. In this notebook we will focus on the sustained phonation (vowel) sound samples. The existing literature in vowel phonetics will help us interpret the estimated feature values. The different acoustic features are: . intensity | pitch or fundamental frequency | formant frequencies | harmonic-to-noise ratio (HNR) | . Extraction procedure . Given a sound sample (or here, an audio file), we first extract these for every 25 msec short-time segment obtained at hops of 10 msec. We used the Parselmouth python package (which uses Praat within it) for this. As an example, you can see the intensity feature vector plotted in the above plot. You can see that regions of low intensity correspond to (close to) silence regions in the sound signals. We use this observation to select short-time segments corresponding to voice activity. You can guess that intensity can be high even for a loud background noise, not coming from the vocal tract. Hence, together with intensity, we also use pitch and HNR values to decide voice activity regions. Finally, we will compute the average of each of these features over all voice-activity detected short-time segments. . File duration before and after VAD . The figure below shows the file duration of the sound samples from close to 650 individuals. Here we have considered only the clean audio files (annotated by human listeners, described in the Metadata post). In the scatter plot you can also see the reduced file duration post VAD. The thresholds for intensity, pitch and HNR are mentioned in the figure and chosen experimentally after observations on a set of files and knowledge from phonetics. Every sound file has some voiceless activity hence, the data points lie below y=x line. We can see that some sound files have a duration greater than 25 sec!, and this is true even after VAD. We also confirmed this by listening to these files, some individuals are really amazing at sustaining their phonation. . . Distribution of Pitch values . The signal processing quantification of perceived pitch of a voice signal is obtained by computing the fundamental frequency of the assumed harmonic complex in the signal. The origin of this harmonic complex in the voice signal owes to the vibrations (opening and closing) of the vocal folds in the glottis (shown in the figure below). Below we show the average pitch distribution obtained by pooling all the sound sample files (each file belongs to a different individual). There is a clear distinction in the peaks of the distribution for the two gender (male/female). Males have a lower pitch than females (to know more on why, see our post on What&#39;s inside the coswara database). The distribution is not very different for the three vowels. This is expected from the insights on voice production. Pitch is more dependent on vibration of glottis and less dependent on the vocal tract configurations. . . Distribution of Formant values . We tracked the first two formants for every sound file. In the plot below we depict the distribution of average values of this obtained by pooling the features from all the sound files. All the distributions are unimodal. These distributions are somewhat different w.r.t. peak location for the three vowels. This is because of the distinct vocal tract configurations associated with each of the vowels. . Distribution of HNR values . The average harmonic-to-noise ratio (HNR) can help quantify the clarity of voicing in each file. A HNR = 0 dB will indicate both noise and speech have equal energy. A HNR&gt;20 dB indicates a good voicing, the higher the better. Hoarse voice has lower HNR (&lt; 20 dB) . Although not shown here, HNR is sensitive to vowel sounds. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/2020/08/20/feature_extraction.html",
            "relUrl": "/coswara/tutorial/2020/08/20/feature_extraction.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Coswara Database_a closer look",
            "content": "About the database . We have released a web application for data collection via crowdsourcing. Essentially, this way anyone with a mobile phone and internet connectivity can contribute to the dataset. The website link can be accessed here. We target participants from following three populations: . healthy: these are individuals with no respiratory illness | unhealthy: these are individuals with respiratory illness | COVID-19 positive: these are individuals identified as COVID-19 positive after RT-PCR test | . Metadata description . When a user opens the website, s/he is asked to fill a short questionnaire which helps us collect metadata to categorize the user into one of the above three populations. The complete metadata is composed of age, gender, location (country, state/province), current health status (healthy / exposed / cured / infected) and the presence of comorbidity (pre-existing medical conditions) information. We do not collect any personally identifiable information. Each user gets a unique anonymized ID during data storage. A screenshot of the coswara webpage is provided below. Here, page 1 and 2 correspond to the metadata collection. . . Sound sample description . In the screenshot shown above, page 3 corresponds to the &quot;audio sample collection&quot; step. We collect audio samples corresponding to nine categories shown in the figure below. These categories were chosen to capture sound signals which embed in them most of the attributes of the respiratory system associated with speech production. To understand how, let&#39;s take a small detour to the speech production system. . Art of speaking . The human speech production system draws contributions from diaphragm, lungs, trachea, larynx, pharynx, tongue, nasal cavity, and lips. You may note that many of these organs are not solely dedicated for speech production, for example, we use mouth for eating and lungs to purify the air! Speech and vocal sound production is an extra feat achieved, thanks to evolution, by these organs. . Lungs have elastic properties, as they are in some sense repurposed swim bladders. During normal respiration, the diaphragm and the abdominal muscles between the ribs work together to expand the lungs. The elastic recoil of the lungs then provides the force that expels air during expiration. This means that the alveolar air pressure increases when you inhale and decreases when you exhale. Something different happens when you speak. . Speaking happens during exhaling. The alveolar air pressure is released gradually in a coordinated manner via the opening and closing of the vocal cords (in the glottis). Something interesting happens here. For voiced sounds, such as vowels, the vocal folds open and close in a periodic fashion. This rate of opening and closing results in imparting periodicity to the output sound pressure wave. Further, this periodicity is one of the easiest perceived attributes in speech, and is referred to as the pitch of the speaker. You would have noticed that male speakers usually have lower pitch than female speakers, and female speakers have lower pitch than kid speakers. Why so? This is related to mass of the vocal folds. Heavier mass means lower pitch, and male anatomy often reveals a higher mass of the vocal folds. But that does not mean you cannot change your pitch. You can by altering the tension of the vocal folds, and we often do this when we want to emphasize something in our speech. Another attribute of speech we perceive quite easily is loudness. Increase in airflow from the lungs blows the vocal folds wider apart resulting in increased strength of the output pressure wave, thus making the sound louder. Voiced sounds are just one category of speech sounds. For unvoiced sounds, such as fricatives, the vocal folds remain open, and for stop consonant sounds, the vocal folds remain closed. Note that these sounds do not have any perceived pitch associated with them. The below figure shows a schematic of the human speech production system. . . What happens during coughing? The textbook explaination suggests that cough is a reflex action. The diaphragm contracts, creating a negative pressure around the lung, and the glottis opens. This enables air to rush into the lungs in order to equalise the pressure. The glottis closes and the vocal cords contract to shut the glottis. The abdominal muscles contract to accentuate the action of the relaxing diaphragm, simultaneously, the other expiratory muscles contract. These actions increase the pressure of air within the lungs. The vocal cords relax and the glottis opens, releasing air at over 100 mph. The bronchi and non-cartilaginous portions of the trachea collapse to form slits through which the air is forced, which clears out any irritants attached to the respiratory lining. So a single cough will have no periodic opening and closing of vocal folds, unlike in vowels. However, often natural coughing results in a sequence of 3-4 coughs, and the physiological description of the opening and closing of glottis can become difficult to describe. An attempt to understand this is made here . What happens during breathing? The glottis largely remains open to enhance free flow of air into and out from the lungs, co-ordinated by the movement of the diaphragm and elasticity of the lungs. A nice video is shown here. . Why nine sound categories . As discussed above, we ask every user to record and upload nine sound samples. These can be grouped as follows: . breathing (two kinds; shallow and deep) | coughing (two kinds; shallow and heavy) | . The choice of the above two is driven by the reporting by WHO and CDC which have listed dry cough, difficulty in breathing, and chest pain (or pressure) as key symptoms of this viral infection, visible between 2-14 days after exposure to the virus. Also, a recent modeling study of symptoms data collected from a pool of 7178 COVID-19 positive individuals validated the presence of these symptoms, and proposed a real-time prediction and tracking approach. Repeated coughing can adversely impact the mass and tension in the vocal folds. This can in turn alter the speaking style of the patient. You might have noticed that you can make a guess if your friend has cold his/her speaking style over phone. . sustained vowel phonation (three kinds; /ey/~as in made, /i/~as in beet, /u:/ as in cool) | . The chosen vowels have a special place in the quantal theory of speech. These vowels are easy to produce and appear almost in every spoken language. Further, these vowel sounds are perceived as most distinct amongst all other vowels, and have been argued to capture the vocal tract attributes effectively. For more details see here and here. . one to twenty digit counting (two kinds; normal and fast paced) | . Counting a sequence of digits corresponds to continuous speaking for close to 20 secs. Any breathing difficulty will make this task difficult, and we expect this to reflect in the speaking style such as loudness, stress and pause patterns, and pace of speaking. . Visualizing the waveforms . In the figure below we show an illustration of the waveforms and the corresponding spectrograms of few sound samples. The waveforms represent the recorded time-domain signal. Here, the spectrogram depicts the spectral content of the signal in every 10 msec short-time window of the signal. We can make some observations from the shown plots. . . The breathing samples are wideband. The spectral energy is distributed over all frequencies. The inhale is lower in energy than the exhale however, both lasts for a similar time-span, close to 1 sec. The exhale (the center burst) also depicts some formant-like structure in the spectrogram. This can be expected as the air travels through the vocal tract. . | For the coughing samples, we can see that these are repeating, and the first cough is a little longer in duration. This can often happen as usually we take a deep breath and release more in the first cough. Also, we can now see some formant structure also in the spectrogram. . | For sustained vowel phonation, we can see clear distinct formant structure, specifically, for the second formant in the spectrogram. . | For the digit counting, we can see the fluctuating formant structure in the spectrogram. . | . It should be noted that these recordings are obtained via crowdsourcing and recorded through web browsers. All sound samples are recorded at 48 kHz in WAV file format. Some of these recordings may have ambient noise which cannot be filtered while recording. In another post we will try quantifying different artifacts we observe in these files. We manually listen to every uploaded file, and will share our opinion on the quality and curation procedure. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/17/closerlook.html",
            "relUrl": "/coswara/2020/08/17/closerlook.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Coswara Categorywise Features",
            "content": "We are in the process of obtaining sound activity annotations for every sound file in the COSWARA database via human listening. Below is some analysis of the 286 annotated sound files. An example of annotations is shown below. The blue shade corresponds to the human annotation. The red trace corresponds to intensity obtained from PRAAT. . . File duration vs. Activity duration . Intensity . We compute the intensity over short-time segments (40 msec, and hop by 1 sample) using PRAAT. A higher inensity value indicates louder sound. We show comparison between segments annotated as activity and no activity segments, with 60 sound files (30 healthy and 30 COVID). . For the below plot we pooled annotated segments from all sound categories. | . . For the below plot we show a categoy wise cmparison. Also, we show two groups, healthy and COVID, for each sound category. The below plot is for comparison of activity. . | Similar to above but comparison of noise (or background) activity. . | . Harmonic-to-noise ration (HNR) of segments . We compute HNR over short-time segments (40 msec, and hop by 1 sample) using PRAAT. A larger HNR value indicates large harmonic content. We show comparison between segments annotated as activity and no activity segments, with 60 sound files (30 healthy and 30 COVID). . For the below plot we pooled annotated segments from all sound categories. | . . For the below plot we show a categoy wise cmparison. Also, we show two groups, healthy and COVID, for each sound category. The below plot is for comparison of activity. . | Similar to above but comparison of noise (or background) activity. . | . Zero-crossing rate (ZCR) of segments . We compute the intensity over short-time segments (25 msec, and hop by 10 msec) using Librosa. This should lie between 0 and 1 (indicating the fraction of zero-crossings in the time window). We show comparison between segments annotated as activity and no activity segments, with 60 sound files (30 healthy and 30 COVID). . For the below plot we pooled annotated segments from all sound categories. | . . For the below plot we show a categoy wise cmparison. Also, we show two groups, healthy and COVID, for each sound category. The below plot is for comparison of activity. . | Similar to above but comparison of noise (or background) activity. . | . Spectral Flatness of segments . We compute the intensity over short-time segments (25 msec, and hop by 10 msec) using Librosa. This should lie between 0 and 1 (closer to 1 indicating a flat spectrum). We show comparison between segments annotated as activity and no activity segments, with 60 sound files (30 healthy and 30 COVID). . For the below plot we pooled annotated segments from all sound categories. | . . For the below plot we show a categoy wise cmparison. Also, we show two groups, healthy and COVID, for each sound category. The below plot is for comparison of activity. . | Similar to above but comparison of noise (or background) activity. . | . Intensity overlaid with human annotations . Sound Category Healthy COVID-19 . cough shallow | Click here | Click here | . cough heavy | Click here | Click here | . vowel-o | Click here | Click here | . vowel-e | Click here | Click here | . vowel-a | Click here | Click here | . breathing-shallow | Click here | Click here | . breathing-deep | Click here | Click here | . counting-normal | Click here | Click here | . counting-fast | Click here | Click here | . Classifying actvity and no activity regions . Using GMM . Using Random Forest . Using Logitic Regression . Spectrograms . Narrowband | . Sound Category Healthy COVID-19 . cough shallow | Click here | Click here | . cough heavy | Click here | Click here | . vowel-o | Click here | Click here | . vowel-e | Click here | Click here | . vowel-a | Click here | Click here | . breathing-shallow | Click here | Click here | . breathing-deep | Click here | Click here | . counting-normal | Click here | Click here | . counting-fast | Click here | Click here | . Wideband | . Sound Category Healthy COVID-19 . cough shallow | Click here | Click here | . cough heavy | Click here | Click here | . vowel-o | Click here | Click here | . vowel-e | Click here | Click here | . vowel-a | Click here | Click here | . breathing-shallow | Click here | Click here | . breathing-deep | Click here | Click here | . counting-normal | Click here | Click here | . counting-fast | Click here | Click here | . Harmonic to noise ratio, overlaid with human annotations . Sound Category Healthy COVID-19 . cough shallow | Click here | Click here | . cough heavy | Click here | Click here | . vowel-o | Click here | Click here | . vowel-e | Click here | Click here | . vowel-a | Click here | Click here | . breathing-shallow | Click here | Click here | . breathing-deep | Click here | Click here | . counting-normal | Click here | Click here | . counting-fast | Click here | Click here | .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/17/category_feats.html",
            "relUrl": "/coswara/2020/08/17/category_feats.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Visualizing sound signals",
            "content": "Before the digital revolution, which started in the 1960s, sound was predominantly recorded by etching the pressure waveform on a physical medium. During playback the etched waveform was tracked and converted to vibrations of a diaphragm. This is a beautiful idea, and you will be amazed to know that the quality was very good. To know more try seeing this video. . In current times, having witnessed the digital revolution, sound signals are stored as discrete sequences of numbers in physical mediums such as semiconductors. This approach is more efficient compared to vinyl records, and hence, have made the capture and playback of sound signals easily accessible. Your mobile phone does it every time you are talking on the phone. When it comes to digital capture of an analogue signal, following needs some understanding, . sampling frequency (fs): As the name implies, it refers to picking a few samples from a continuous-time signal (or waveform). Sampling frequency refers to the number of samples you pick per second, samples being spaced uniformly apart in time. Obviously, while you are sampling you are also discarding a lot. Is there a critical sampling frequency when the discarding is not going to hurt you? Yes, and this rate is referred to as the Nyquist-rate. It is equal to the twice the maximum frequency content in the continuous-time signal you are interested to sample. In our case this signal of interest is sound, and a good choice of fs is 48 kHz for music signals and 16 kHz for speech signals. A beautiful illustration of this is provided here and more details here . | quantization: Once we have sampled in time, the next step is to store the amplitude values taken by the signal at the sampled time instants. These amplitude values will be stored in the computer (or more specifically, disk drives) and this storage has a finite resolution for storing a number decided by the number of bits used to represent a number. These bits can be n = 2,4,8, 16, 32, etc. For n bits, the resolution is (1/(2^n-1)). So, more the number of bits, higher is the resolution and hence, the representation (or storage) of the number in the memory of the computer will be more accurate. Sound (or audio) is usually stored at 16-bits. For more details you can read this). . | . Hurray, a sampled and quantized analogue signal can be stored in any digital media. And once we have stored it, we can also read (or load) it back from the digital media! In step 1, we will attempt reading (or loading) a sound file stored in the github server, and visualize (and hear) the content inside it. . Loading a sound file . We will load a file &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39;. Any WAV file has some metadata stored inside it. This metadata gives information about the sampling frequency, quantization bits, number of channels, etc., used during the capture (and) storage of the sound file. Below we show a screenshot of this information for our file. . . We can see that the fs is 16 kHz. Lets now load, listen, and plot the data inside this sound file. . #collapse import numpy as np import librosa from IPython.lib.display import Audio import matplotlib.pyplot as plt from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable import seaborn as sns fs = 16000 fname = &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39; dname = &#39;./my_data/&#39; # load x, sr = librosa.load(dname+fname,sr=16000) x = x/max(np.abs(x)) times = np.arange(0,len(x))/fs # listen Audio(x, rate=sr, autoplay=False) . . Your browser does not support the audio element. Sound signal as a time-series . On listening using the above widget you would have recognized the sound as repeated coughing. Lets plot the signal. . #collapse # plot fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,1,1) ax.plot(times, x) ax.grid(True) plt.ylabel(&#39;amplitude [in A.U.]&#39;, fontsize=14) plt.xlabel(&#39;time [in sec]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . The signal starts with silence, and around 0.5 sec we see the start of the first cough. This is followed by two more, and then a pause which might be an inhaling of air. Subsequently. we see three more coughs. Lets visualize the distribution of the sample values. . #collapse_show fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,2,1) ax.hist(x,bins=1000,range=(x.min(), x.max())) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xlabel(&#39;amplitude [in A.U]&#39;, fontsize=14) # plt.xticks(fontsize=13) # plt.yticks(fontsize=13) plt.xlim(-.1,.1) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.scatter(x[:-1],x[1:]) ax.grid(True) plt.ylabel(&#39;x [n+1]&#39;, fontsize=14) plt.xlabel(&#39;x [n]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . We see that the histogram (shown on the left) peaks around zero. This means most of the sample values are low amplitude (&lt;.0125). We also plot a phase plot of x[n+1] vs x[n] on the right. This lies along y=x indicating a correlation between consecutive time samples. Such high correlation also implies high low frequency (relative to 8 kHz) content in the signal. We will verify this observation using Fourier transform. . Spectrum of a sound signal . A time-domain signal can be analyzed using Fourier transform. (To be fair, any signal can be analyzed using Fourier transform). Via a Fourier transform we can visualize the frequency (or spectral) content in the signal. This is useful for sound signal analysis. The obtained spectral content can help us understand certain perceived attributes of the sound (namely, timbre). Lets compute and visualize the spectrum of the above sound signal. . #collapse def nearestpow2(n): k=1 while n&gt;2**k: k = k+1 return 2**k nfft = nearestpow2(len(x)) X = np.fft.rfft(x,nfft) freq = np.arange(0,nfft/2+1)/nfft*fs #collapse_show fig = plt.subplots(figsize=(16,5)) ax = plt.subplot(1,2,1) ax.plot(freq,np.abs(X)) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in A.U]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.plot(freq,20*np.log10(np.abs(X))-np.max(20*np.log10(np.abs(X)))) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in dB]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.ylim(-60,10) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) . . The plot on the left depicts the spectrum distributed from 0 to 8 kHz (=fs/2). We see a peak (like of a mountain) around 300 Hz, and a second peak around 1500 Hz, followed by three more peaks. Also, there is a roll-off of the spectral amplitude from lower to higher frequencies. To decrease the contrast between the too high peak and other smaller peaks we can apply a dB transformation to the spectrum amplitude. The resulting plot is shown on the right. Applying dB transform for visualizing the spectral content also makes sense from perception aspects (you might have noticed sound pressure is reported in dBs). . Spectrograms . The Fourier transform helps us understand the frequency content (spectrum) of this sound signal. This is useful. Does the ear also use a similar approach to analyze a sound signal? Scientists have dissected mammalian ears and found that the organ inside the ear (referred to as the cochlea) acts to certain extent like a mechanical Fourier analyzer. In relation do check out this video. But definitely the cochlea will not wait for the whole signal to end (like in our case 3.5 secs) and then compute the Fourier transform. We hear the sound within 10 msecs. While you are listening to a speech, you don&#39;t wait for the person to finish to understand the speech, innstead you start understanding it while the person is speaking. Thus, lets take the Fourier transform of small segments of the sound signal. But how small? You are free to choose any length. . We will use something in between 10-30 msec and the reasoning is that if the minimum frequency in the sound signal is 50 Hz then we would have at least captured 2 cycles. | This also relates to non-stationarity in the signal. Sound signals such as speech and music have a time-varying spectral content. Hence, it makes sense to analyze the signal in short-time segments. | . Don&#39;t worry if the above is not clear to you. Below is an illustration on the same. Let&#39;s understand this. From the speech signal you take a 25 msec segment, compute its magnitude Fourier transform, and push the output into a column of an empty matrix. Then you hop in time by 10 msec, and again take a 25 msec segment from the speech signal, and repeat the same thing, that is, compute magnitude Fourier transform and push the output into the next column of the matrix. This way you move from the start to end of the signal by hopping in 10 msecs. The matrix you thus obtained is called the spectrogram. It is plotted as an image (color bar: the gradient from dark blue to white indicates high to low amplitude in the spectral content). Do you like this image? You can notice some beautiful horizontal striations. These correspond to harmonic frequencies in certain time-segments in the speech signal. Some amazing folks can read out a sentence by just staring at the spectrogram. Check out this for more details. . Now that we know about spectrogram, let&#39;s compute this for our cough signal. . #collapse # first we define the spectrogram function def generate_spectrogram(x,fs,wdur=20e-3,hdur=5e-3): X = [] i = 0 cnt = 0 win = np.hamming(wdur*fs) win = win - np.min(win) win = win/np.max(win) while i&lt;(len(x)-int(wdur*fs)): X.append(np.multiply(win,x[i:(i+int(wdur*fs))])) i = i + int(hdur*fs) cnt= cnt+1 X = np.array(X) Xs = abs(np.fft.rfft(X)) return Xs # lets plot now fig = plt.subplots(figsize=(6,1)) ax = plt.subplot(1,1,1) ax.plot(times,x) ax.set_xlim(times[0],times[-1]) ax.set_ylim(-1,1) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U&#39;) sns.despine(offset = .1,trim=False) plt.show() fig, ax = plt.subplots(figsize=(6,4)) Xs = generate_spectrogram(x,fs,wdur=10e-3,hdur=2.5e-3) XdB = 20*np.log10(Xs.T) XdB = XdB - np.max(XdB) im = ax.imshow(XdB,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,extent = [times[0], times[-1], 0, fs/2/1e3], cmap=&#39;RdBu_r&#39;,vmin = 0, vmax =-100) divider = make_axes_locatable(ax) colorbar_ax = fig.add_axes([.95, 0.1, 0.015, 0.5]) fig.colorbar(im, cax=colorbar_ax) ax.set_xlim(times[0],times[-1]) # ax.set_xlim(.2,3) ax.set_ylim(-.1,4) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;FREQ [in kHz]&#39;) sns.despine(offset = 0.01,trim=False) plt.show() . .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/2020/08/16/sound_visualization.html",
            "relUrl": "/coswara/tutorial/2020/08/16/sound_visualization.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Coswara metadata visualization",
            "content": "The coswara database is created by crowdsourcing respiratory sound samples. In this post we will visualize the crowd (or participant) distribution along certain dimensions collected in the metadata questionnaire. We will read the CSV file containing the metadata information of all the users (as on 07 August 2020). From the whole dataset, we have manually listened to 941 participants&#39; audio samples. Below we present this data. A more detailed documentation is also available here and will be presented at the Interspeech 2020 conference. . First we visualize the gender, age, and country-wise (India/outside) distribution. . #collapse # import some packages import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) # load CSV file fname = &#39;combined_plus_annotated_IS2020.csv&#39; DF = pd.read_csv(&#39;./my_data/&#39;+fname) # plot gender information gender_labels = DF[&#39;g&#39;].unique() gender_cnt = [] for i in range(len(gender_labels)): gender_cnt.append(len(DF[(DF[&#39;g&#39;] == gender_labels[i]) &amp; DF[&#39;cough-heavy-quality&#39;]])) fig = plt.subplots(figsize=(16, 4)) ax = plt.subplot(1,3,1) ax.bar(2,gender_cnt[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) ax.bar(4,gender_cnt[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot;//&quot;,color=&#39;blue&#39;,width=.5) for i, v in enumerate(gender_cnt): ax.text(2*(i+1)-.2,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) plt.xticks([2,4], [&#39;MALE&#39;,&#39;FEMALE&#39;],rotation=0) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) # ax.set_xlim(0,5) # ax.set_ylim(200,1500) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plot age information age_labels = DF[&#39;a&#39;].unique() age_cnt = [] for i in range(len(age_labels)): age_cnt.append(len(DF[(DF[&#39;a&#39;] == age_labels[i])])) ax = plt.subplot(1,3,2) ax.bar(age_labels,age_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,color=&#39;blue&#39;) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xlabel(&#39;AGE&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plot country information country_labels = DF[&#39;l_c&#39;].unique() country_cnt = [] for i in range(len(country_labels)): country_cnt.append(len(DF[DF[&#39;l_c&#39;] == country_labels[i]])) country_cnt = np.array(country_cnt) indx = np.argsort(country_cnt)[::-1] country_cnt = country_cnt[indx] country_labels = country_labels[indx] two_categories = [country_cnt[0],np.sum(country_cnt[1:])] two_labels = [&#39;INDIA&#39;,&#39;OTHERS&#39;] ax = plt.subplot(1,3,3) ax.bar(2,two_categories[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) ax.bar(4,two_categories[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) plt.xticks([2,4],two_labels,rotation=0) for i, v in enumerate(two_categories): ax.text(2*(i+1)-.25,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . India has 29 states. Next, let&#39;s visualize the across state distribution. We show only the top 5 below. . #collapse state_labels = DF[&#39;l_s&#39;].unique() state_cnt = [] for i in range(len(state_labels)): state_cnt.append(len(DF[DF[&#39;l_s&#39;] == state_labels[i]])) state_cnt = np.array(state_cnt) indx = np.argsort(state_cnt)[::-1][0:6] state_cnt = state_cnt[indx] state_labels = state_labels[indx] fig, ax = plt.subplots(figsize=(8, 4)) ax.bar(np.arange(0,len(state_cnt)),state_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.25) ax.set_ylabel(&#39;PARTICIPANT COUNT&#39;,fontsize=14) # # ax.text(1.5,-9,&#39;MEAN&#39;,horizontalalignment=&#39;center&#39;) plt.xticks(np.arange(0,len(state_cnt)),state_labels,rotation=30,fontsize=13) for i, v in enumerate(state_cnt): ax.text(i-.15,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . Next, we group the participants into two groups, healthy and unhealthy. This grouping is based on absence/presence of any respiratory ailments reported in the metadata questionnaire. . #collapse labels = [&#39;HEALTHY&#39;,&#39;UNHEALTHY&#39;] category_cnt = [] category_cnt.append(len(DF.loc[(DF[&#39;covid_status&#39;]==&#39;healthy&#39;) &amp; (DF[&#39;asthma&#39;]!=True)&amp; (DF[&#39;cld&#39;]!=True)&amp; (DF[&#39;cold&#39;]!=True)&amp; (DF[&#39;cough&#39;]!=True)&amp; (DF[&#39;pneumonia&#39;]!=True)&amp; (DF[&#39;fever&#39;]!=True)])) category_cnt.append(len(DF.loc[(DF[&#39;covid_status&#39;]==&#39;resp_illness_not_identified&#39;) | (DF[&#39;covid_status&#39;]==&#39;positive_mild&#39;) | (DF[&#39;asthma&#39;]==True)| (DF[&#39;cld&#39;]==True)| (DF[&#39;cold&#39;]==True)| (DF[&#39;cough&#39;]==True)| (DF[&#39;pneumonia&#39;]==True)| (DF[&#39;fever&#39;]==True)])) fig = plt.subplots(figsize=(4,4)) ax = plt.subplot(1,1,1) ax.bar(2,category_cnt[0],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5,label=&#39;clean&#39;) ax.bar(4,category_cnt[1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5,label=&#39;noisy&#39;) plt.xticks([2,4],labels,rotation=0) for i, v in enumerate(category_cnt): ax.text(2*(i+1)-.25,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() plt.show() . . Lastly, we have listened to all the 941 participants&#39; audio data and classified every audio file into clean, noisy, and bad (highly degraded). The resulting distribution across the nine sound categories is shown below. . #collapse labels = [&#39;vowel-o&#39;,&#39;vowel-e&#39;,&#39;vowel-a&#39;,&#39;cough-shallow&#39;,&#39;cough-heavy&#39;,&#39;breathing-shallow&#39;,&#39;breathing-deep&#39;, &#39;counting-normal&#39;,&#39;counting-fast&#39;] category_cnt = [] for label in labels: category_cnt.append(len(DF[(DF[label]==label) &amp; ((DF[label+&#39;-quality&#39;]==&#39;clean_audio&#39;)) &amp; ((DF[label+&#39;-cont&#39;]==&#39;y&#39;)) &amp; (DF[label+&#39;-vol&#39;]==&#39;y&#39;)])) category_cnt.append(len(DF[(DF[label]==label) &amp; ((DF[label+&#39;-quality&#39;]==&#39;noisy_audio&#39;)) &amp; ((DF[label+&#39;-cont&#39;]==&#39;y&#39;))])) category_cnt.append(len(DF[(DF[label]==label) &amp; (((DF[label+&#39;-quality&#39;]==&#39;bad_audio&#39;)) |((DF[label+&#39;-quality&#39;]==&#39;clean_audio&#39;)&amp;(DF[label+&#39;-cont&#39;]==&#39;n&#39;)) |((DF[label+&#39;-quality&#39;]==&#39;noisy_audio&#39;)&amp;(DF[label+&#39;-cont&#39;]==&#39;n&#39;)) )])) fig = plt.subplots(figsize=(10,4)) ax = plt.subplot(1,1,1) cnt = 0 indx = 0 xticks = [] for i in range(len(category_cnt)//3): if i ==0: ax.bar(cnt,category_cnt[indx],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=1,label=&#39;clean&#39;) ax.bar(cnt+1,category_cnt[indx+1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,width=1,label=&#39;noisy&#39;) ax.bar(cnt+2,category_cnt[indx+2],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;green&#39;,width=1,label=&#39;bad&#39;) else: ax.bar(cnt,category_cnt[indx],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=1) ax.bar(cnt+1,category_cnt[indx+1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,width=1) ax.bar(cnt+2,category_cnt[indx+2],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;green&#39;,width=1) xticks.append(cnt+.5) cnt = cnt+4 indx = indx+3 ax.set_xticks(xticks) ax.set_xticklabels(labels,rotation=30,fontsize=13) ax.grid(True) ax.set_xlim(-2,cnt+2) ax.legend(loc=&#39;upper right&#39;,frameon=False,bbox_to_anchor=(1.05,1),fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.yticks(fontsize=13) plt.show() . . In another post we will attempt to describe the acoustic features of the audio samples. Looking forward to having you with us in this exploration. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/15/visualize_metadata.html",
            "relUrl": "/coswara/2020/08/15/visualize_metadata.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Speech signals - a wonder",
            "content": "Some context . &quot;Tring, tring&quot;,... your phone is ringing. You pick it, and hear - &quot;Hello, what&#39;s up?&quot;. Often, this sound, pouring into your ears, will inform you about the gender of the talker, the identity, and the emotion. Isn&#39;t this fascinating? With only 2-3 secs of sound signal, the human brain has estimated so much about the talker. As the conversation progresses, you will be able to even estimate the personality, and the health status of the talker. Speech signal is immensely rich in information, and our brain is trained, since our childhood, to extract a lot from it. . Speech machinery . Let&#39;s see how this fascinating sound signal -speech, is produced. You inhale a breath, the air enters your lungs and creates a high pressure. When you speak, a sequence of co-ordinated mechanical processes are initiated, flexing the vocal tract muscles, and the result is the release of air pressure from mouth and nose. If you don&#39;t believe this - just pause - take a deep breath, and read aloud the previous line. Didn&#39;t you exhale while reading it! The co-ordinated mechanical processes involved during speaking are a miracle. Just to give a context, human beings are the only species on earth which can produce the diverse range of acoustic sounds making up our vocal communication repository. Biology suggests that the reason is linked to the FOXP2 gene - found only in humans. Every human is unique, and this uniqueness also reflects in the speech signals which allows us to easily recognize the voice of many. . Recording speech . Satisfying the human curiosity to record speech signals was a challenge. How do you store the speech sounds? Speak into a jar, close the lid, and open it, and bingo, you hear it! Sorry, physics won&#39;t allow this to happen. In the late 19th century, phonograph was invented, a beautiful mind behind this was Thomas Edison. As time progressed, this became popular to record music, and lead to gramophone, magnetic tape cassettes, and compact discs. Now we just store it in solid state semiconductor drives without worrying about how! Technology has been a blessing when you consider the seamless manner in which we capture, store, process, and playback speech, music, and images. . Processing speech . Cool! We are able to record sound signals. Let&#39;s move on to processing sound signals. Can we design machine systems which extract information from sound signals? For instance, can machines perform speech recognition, speaker recognition, emotion recognition, and ... the list can go on. Our brains do all this. It shouldn&#39;t be impossible to design machines to do this, and beyond this too! Well, automatic speech recognition is now a reality, and accessible in our mobile phones/laptops. Challenges exist when the speech recording is noisy, accented, not in English, or has multiple talkers etc. For single talkers, clean recording, American/British accent, English speech, the machine systems work quite well. Similar is the performance for talker recognition as well. What is the key behind this technology? Welcome to the world of signal processing and machine learning! .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/06/20/speech_signals.html",
            "relUrl": "/coswara/2020/06/20/speech_signals.html",
            "date": " • Jun 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This blog shares our journey in the Coswara project. The project is launched by the LEAP lab, based at the Indian Institute of Science, Bangalore. .",
          "url": "https://iiscleap.github.io/coswara-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://iiscleap.github.io/coswara-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}