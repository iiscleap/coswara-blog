{
  
    
        "post0": {
            "title": "Feature Extraction",
            "content": "With the advent of well engineered sensors, the practice of data collection has gained pace. Individuals and groups have put significant efforts to record and host hours of audio recordings on the internet. Also, many of these are accessible at the click of a mouse. Is all data also information? Information is always for a purpose, and hence, the proper question will be - is this data informative for so and so task? To answer this we have to go beyond just visualizing sound signals, and understand the concept of features. . Example: How do you distinguish tomatoes from potatoes, without tasting? Color (red vs brown), shape (close to spherical vs somewhat spherical), tightness (soft vs hard), etc. What we listed are the features of vegetables, and values these features take can help us distinguish one vegetable from other. How? In the figure shown below. . #collapse_show import numpy as np import matplotlib.pyplot as plt x = [] x.append(np.random.randn(100)) x.append(2*np.random.randn(100)+5) plt.hist(x[0],color=&#39;r&#39;) plt.hist(x[1],color=&#39;b&#39;) plt.show() . . #collapse_show import numpy as np import librosa from IPython.lib.display import Audio import matplotlib.pyplot as plt from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable import seaborn as sns fs = 16000 fname = &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39; dname = &#39;./my_data/&#39; # load x, sr = librosa.load(dname+fname,sr=16000) x = x/max(np.abs(x)) times = np.arange(0,len(x))/fs # listen Audio(x, rate=sr, autoplay=False) . . Your browser does not support the audio element. Sound signal as a time-series . On listening using the above widget you would have recognized the sound as repeated coughing. Lets plot the signal. . #collapse_show # plot fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,1,1) ax.plot(times, x) ax.grid(True) plt.ylabel(&#39;amplitude [in A.U.]&#39;, fontsize=14) plt.xlabel(&#39;time [in sec]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . The signal starts with silence, and around 0.5 sec we see the start of the first cough. This is followed by two more, and then a pause which might be an inhaling of air. Subsequently. we see three more coughs. Lets visualiza the distribution of the sample values. . #collapse_show fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,2,1) ax.hist(x,bins=1000,range=(x.min(), x.max())) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xlabel(&#39;amplitude [in A.U]&#39;, fontsize=14) # plt.xticks(fontsize=13) # plt.yticks(fontsize=13) plt.xlim(-.1,.1) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.scatter(x[:-1],x[1:]) ax.grid(True) plt.ylabel(&#39;x [n+1]&#39;, fontsize=14) plt.xlabel(&#39;x [n]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . We see that the histogram (shown on the left) peaks around zero. This means most of the sample values are low amplitude (&lt;.0125). We also plot a phase plot of x[n+1] vs x[n] on the right. This lies along y=x indicating a correlation between consecutive time samples. Such high correlation also implies high low frequency (relative to 8 kHz) content in the signal. We will verify this observation using Fourier transform. . Spectrum of a sound signal . A time-domain signal can be analyzed using Fourier transform. (To be fair, any signal can be analyzed using Fourier transform). Via a Fourier tranform we can visualize the frequency (or spectral) content in the signal. This is useful for sound signal analysis. The obtained spectral content can help us understand certain perceived attributes of the sound (namely, timbre). Lets compute and visualize the spectrum of the above sound signal. . #collapse_show def nearestpow2(n): k=1 while n&gt;2**k: k = k+1 return 2**k nfft = nearestpow2(len(x)) X = np.fft.rfft(x,nfft) freq = np.arange(0,nfft/2+1)/nfft*fs #collapse_show fig = plt.subplots(figsize=(16,5)) ax = plt.subplot(1,2,1) ax.plot(freq,np.abs(X)) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in A.U]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.plot(freq,20*np.log10(np.abs(X))-np.max(20*np.log10(np.abs(X)))) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in dB]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.ylim(-60,10) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) . . The plot on the left depicts the spectrum distributed from 0 to 8 kHz (=fs/2). We see a peak (like of a mountain) around 300 Hz, and a second peak around 1500 Hz, follwed by three more peaks. Also, there is a roll-off of the spectral amplitude from lower to higher frequencies. To decreases the contrast between the too high peak and other smaller peaks we can apply a dB transformation to the spectrum amplitude. The resulting plot is shown on the right. Applying dB transform for visualizing the spectral content also makes sense from perception aspects (you might have noticed sound pressure is reported in dBs). . Spectrograms . The Fourier tranform helps us understand the frequency content (spectrum) of this sound signal. This is useful. Does the ear also use a similar approach to analyze a sound signal? Scientists have dissected mammaliann ears and found that the organ inside the ear (referred as the cochlea) acts to certain extent like a mechanical Fourier analyzer. In relation do check out this video. But definitely the cochlea will not wait for the whole signal to end (like in our case 3.5 secs) and then compute the Fourier transform. We hear the sound within 10 msecs. While you are listening to speech, you don&#39;t wait for the person to finish to understand the speech, innstead you start understanding it while the person is speaking. Thus, lets take Fourier transform of small segments of the sound signal. But how small? You are free to choose any length. . We will use something in between 10-30 msec and the reasoning is that if the minumum frequency in the sound signal is 50 Hz then we would have atleast captured 2 cycles. | This also relates to non-stationarity in the signal. Sound signal such as speech and music have a time-varying spectral content. Hence, it makes sense to analyze the signal in short-time segments. | . Don&#39;t worry if the above is not clear to you. Below is an illustration on the same. Lets underatand this. From the speech signal you take a 25 msec segment, compute its magnitude Fourier transform, and push the the output into a column of an empty matrix. Then you hop in time by 10 msec, and again take a 25 msec segment from the speech signal, and repeat the same thing, that is, compute magnitude Fourier transform and push the output into the next columnn of the matrix. This way you move from the start to end of the signal by hopping in 10 msecs. The matrix you thus obtained is called the spectrogram. It is plotted as an image (color bar: the gradient from dark blue to white indicates high to low amplitude in the spectral content). Do you like this image? You can notice some beutiful horizontal striations. These correspond to harmonic frequencies in certain time-segments in the speech signal. Some amazing folks can read out a sentence by just starring at the spectrogram. Check out this for more details. . Now that we know about spectrogram, lets compute this for our cough signal. . #collapse_show # first we define the spectrogram function def generate_spectrogram(x,fs,wdur=20e-3,hdur=5e-3): X = [] i = 0 cnt = 0 win = np.hamming(wdur*fs) win = win - np.min(win) win = win/np.max(win) while i&lt;(len(x)-int(wdur*fs)): X.append(np.multiply(win,x[i:(i+int(wdur*fs))])) i = i + int(hdur*fs) cnt= cnt+1 X = np.array(X) Xs = abs(np.fft.rfft(X)) return Xs # lets plot now fig = plt.subplots(figsize=(6,1)) ax = plt.subplot(1,1,1) ax.plot(times,x) ax.set_xlim(times[0],times[-1]) ax.set_ylim(-1,1) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U&#39;) sns.despine(offset = .1,trim=False) plt.show() fig, ax = plt.subplots(figsize=(6,4)) Xs = generate_spectrogram(x,fs,wdur=10e-3,hdur=2.5e-3) XdB = 20*np.log10(Xs.T) XdB = XdB - np.max(XdB) im = ax.imshow(XdB,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,extent = [times[0], times[-1], 0, fs/2/1e3], cmap=&#39;RdBu_r&#39;,vmin = 0, vmax =-100) divider = make_axes_locatable(ax) colorbar_ax = fig.add_axes([.95, 0.1, 0.015, 0.5]) fig.colorbar(im, cax=colorbar_ax) ax.set_xlim(times[0],times[-1]) # ax.set_xlim(.2,3) ax.set_ylim(-.1,4) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;FREQ [in kHz]&#39;) sns.despine(offset = 0.01,trim=False) plt.show() . .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/2020/08/17/feature_extraction.html",
            "relUrl": "/coswara/tutorial/2020/08/17/feature_extraction.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Coswara Database_a closer look",
            "content": "With the advent of well engineered sensors, the practice of data collection has gained pace. Individuals and groups have put significant efforts to record and host hours of audio recordings on the internet. Also, many of these are accessible at the click of a mouse. Is all data also information? Information is always for a purpose, and hence, the proper question will be - is this data informative for so and so task? To answer this we have to go beyond just visualizing sound signals, and understand the concept of features. . Example: How do you distinguish tomatoes from potatoes, without tasting? Color (red vs brown), shape (close to spherical vs somewhat spherical), tightness (soft vs hard), etc. What we listed are the features of vegetables, and values these features take can help us distinguish one vegetable from other. How? In the figure shown below. . #collapse_show import numpy as np import matplotlib.pyplot as plt x = [] x.append(np.random.randn(100)) x.append(2*np.random.randn(100)+5) plt.hist(x[0],color=&#39;r&#39;) plt.hist(x[1],color=&#39;b&#39;) plt.show() . . #collapse_show import numpy as np import librosa from IPython.lib.display import Audio import matplotlib.pyplot as plt from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable import seaborn as sns fs = 16000 fname = &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39; dname = &#39;./my_data/&#39; # load x, sr = librosa.load(dname+fname,sr=16000) x = x/max(np.abs(x)) times = np.arange(0,len(x))/fs # listen Audio(x, rate=sr, autoplay=False) . . Your browser does not support the audio element. Sound signal as a time-series . On listening using the above widget you would have recognized the sound as repeated coughing. Lets plot the signal. . #collapse_show # plot fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,1,1) ax.plot(times, x) ax.grid(True) plt.ylabel(&#39;amplitude [in A.U.]&#39;, fontsize=14) plt.xlabel(&#39;time [in sec]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . The signal starts with silence, and around 0.5 sec we see the start of the first cough. This is followed by two more, and then a pause which might be an inhaling of air. Subsequently. we see three more coughs. Lets visualiza the distribution of the sample values. . #collapse_show fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,2,1) ax.hist(x,bins=1000,range=(x.min(), x.max())) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xlabel(&#39;amplitude [in A.U]&#39;, fontsize=14) # plt.xticks(fontsize=13) # plt.yticks(fontsize=13) plt.xlim(-.1,.1) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.scatter(x[:-1],x[1:]) ax.grid(True) plt.ylabel(&#39;x [n+1]&#39;, fontsize=14) plt.xlabel(&#39;x [n]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . We see that the histogram (shown on the left) peaks around zero. This means most of the sample values are low amplitude (&lt;.0125). We also plot a phase plot of x[n+1] vs x[n] on the right. This lies along y=x indicating a correlation between consecutive time samples. Such high correlation also implies high low frequency (relative to 8 kHz) content in the signal. We will verify this observation using Fourier transform. . Spectrum of a sound signal . A time-domain signal can be analyzed using Fourier transform. (To be fair, any signal can be analyzed using Fourier transform). Via a Fourier tranform we can visualize the frequency (or spectral) content in the signal. This is useful for sound signal analysis. The obtained spectral content can help us understand certain perceived attributes of the sound (namely, timbre). Lets compute and visualize the spectrum of the above sound signal. . #collapse_show def nearestpow2(n): k=1 while n&gt;2**k: k = k+1 return 2**k nfft = nearestpow2(len(x)) X = np.fft.rfft(x,nfft) freq = np.arange(0,nfft/2+1)/nfft*fs #collapse_show fig = plt.subplots(figsize=(16,5)) ax = plt.subplot(1,2,1) ax.plot(freq,np.abs(X)) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in A.U]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.plot(freq,20*np.log10(np.abs(X))-np.max(20*np.log10(np.abs(X)))) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in dB]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.ylim(-60,10) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) . . The plot on the left depicts the spectrum distributed from 0 to 8 kHz (=fs/2). We see a peak (like of a mountain) around 300 Hz, and a second peak around 1500 Hz, follwed by three more peaks. Also, there is a roll-off of the spectral amplitude from lower to higher frequencies. To decreases the contrast between the too high peak and other smaller peaks we can apply a dB transformation to the spectrum amplitude. The resulting plot is shown on the right. Applying dB transform for visualizing the spectral content also makes sense from perception aspects (you might have noticed sound pressure is reported in dBs). . Spectrograms . The Fourier tranform helps us understand the frequency content (spectrum) of this sound signal. This is useful. Does the ear also use a similar approach to analyze a sound signal? Scientists have dissected mammaliann ears and found that the organ inside the ear (referred as the cochlea) acts to certain extent like a mechanical Fourier analyzer. In relation do check out this video. But definitely the cochlea will not wait for the whole signal to end (like in our case 3.5 secs) and then compute the Fourier transform. We hear the sound within 10 msecs. While you are listening to speech, you don&#39;t wait for the person to finish to understand the speech, innstead you start understanding it while the person is speaking. Thus, lets take Fourier transform of small segments of the sound signal. But how small? You are free to choose any length. . We will use something in between 10-30 msec and the reasoning is that if the minumum frequency in the sound signal is 50 Hz then we would have atleast captured 2 cycles. | This also relates to non-stationarity in the signal. Sound signal such as speech and music have a time-varying spectral content. Hence, it makes sense to analyze the signal in short-time segments. | . Don&#39;t worry if the above is not clear to you. Below is an illustration on the same. Lets underatand this. From the speech signal you take a 25 msec segment, compute its magnitude Fourier transform, and push the the output into a column of an empty matrix. Then you hop in time by 10 msec, and again take a 25 msec segment from the speech signal, and repeat the same thing, that is, compute magnitude Fourier transform and push the output into the next columnn of the matrix. This way you move from the start to end of the signal by hopping in 10 msecs. The matrix you thus obtained is called the spectrogram. It is plotted as an image (color bar: the gradient from dark blue to white indicates high to low amplitude in the spectral content). Do you like this image? You can notice some beutiful horizontal striations. These correspond to harmonic frequencies in certain time-segments in the speech signal. Some amazing folks can read out a sentence by just starring at the spectrogram. Check out this for more details. . Now that we know about spectrogram, lets compute this for our cough signal. . #collapse_show # first we define the spectrogram function def generate_spectrogram(x,fs,wdur=20e-3,hdur=5e-3): X = [] i = 0 cnt = 0 win = np.hamming(wdur*fs) win = win - np.min(win) win = win/np.max(win) while i&lt;(len(x)-int(wdur*fs)): X.append(np.multiply(win,x[i:(i+int(wdur*fs))])) i = i + int(hdur*fs) cnt= cnt+1 X = np.array(X) Xs = abs(np.fft.rfft(X)) return Xs # lets plot now fig = plt.subplots(figsize=(6,1)) ax = plt.subplot(1,1,1) ax.plot(times,x) ax.set_xlim(times[0],times[-1]) ax.set_ylim(-1,1) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U&#39;) sns.despine(offset = .1,trim=False) plt.show() fig, ax = plt.subplots(figsize=(6,4)) Xs = generate_spectrogram(x,fs,wdur=10e-3,hdur=2.5e-3) XdB = 20*np.log10(Xs.T) XdB = XdB - np.max(XdB) im = ax.imshow(XdB,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,extent = [times[0], times[-1], 0, fs/2/1e3], cmap=&#39;RdBu_r&#39;,vmin = 0, vmax =-100) divider = make_axes_locatable(ax) colorbar_ax = fig.add_axes([.95, 0.1, 0.015, 0.5]) fig.colorbar(im, cax=colorbar_ax) ax.set_xlim(times[0],times[-1]) # ax.set_xlim(.2,3) ax.set_ylim(-.1,4) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;FREQ [in kHz]&#39;) sns.despine(offset = 0.01,trim=False) plt.show() . .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/17/closerlook.html",
            "relUrl": "/coswara/2020/08/17/closerlook.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Visualizing sound signals",
            "content": "Before the digital revolution, which started in 1960s, sound was predominantly recorded by etching the pressure waveform on a physical medium. During playback the etched waveform was tracked and converted to vibrations of a diaphragm. This is a beautiful idea, and you will be amazed to know that the quality was very good. To know more try seeing this video. . In current times, having witnessed the digital revolution, sound signals are stored as discrete sequence of numbers in in physical mediums such semiconductors. This approch is more efficient compared to vinyl records, and hence, have made the capture and playback of sound signals easily accessible. Your mobile phone does it everytime you are talking on phone. When it comes to digital capture of an analogue signal, following needs some understanding, . sampling frequency (fs): As the name implies, it refers to picking few samples from a continuous-time signal (or waveform). Sampling frequency refers to the number of samples you pick per second, samples being spaced uniformly apart in time. Obviously, you are while you are sampling you are also discarding a lot. Is there a critical sampling frequency when the discarding is not going to hurt you? Yes, and this rate is referred to as the Nyquist-rate. It is equal to the twice the maximum frequency content in the continuous-time signal you are interested to sample. In our case this signal of interest is sound, and a good choice of fs is 48 kHz for music signals and 16 kHz for speech signals. A beautiful illustration of this is provided here and more details here . | quantization: Once we have sampled along time, the next step is to store the amplitude values taken by the signal at the sampled time instants. These ampltude values will be stored in the computer (or more specifically, disk drives) and this storage has a finite resolution for storing a number decided by the number of bits used to represent a number. These bits can be n = 2,4,8, 16, 32, etc. For n bits, the resolution is (1/(2^n-1)). So, more the number of bits, higher is the resolution and hence, the representation (or storage) of the number in the memory of the computer will be more accurate. Sound (or audio) is usually stored at 16-bits. For more details you can read this). . | . Hurray, a sampled and quantized analogue signal can be stored in any digital media. And once we have stored it, we can also read (or load) it back from the digital media! In step 1, we will attempt reading (or loading) a sound file stored in the github server, and visualize (and hear) the content inside it. . Loading a sound file . We will load a file &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39;. Any WAV file has some metadata stored inside it. This metadata gives information about the sampling frequency, quantization bits, number of channel, etc., used during the capture (and) storage of the sound file. Below we show a screenshot of this information for our file. . . We can see that the fs is 16 kHz. Lets now load, listen, and plot the data inside this sound file. . #collapse_show import numpy as np import librosa from IPython.lib.display import Audio import matplotlib.pyplot as plt from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable import seaborn as sns fs = 16000 fname = &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39; dname = &#39;./my_data/&#39; # load x, sr = librosa.load(dname+fname,sr=16000) x = x/max(np.abs(x)) times = np.arange(0,len(x))/fs # listen Audio(x, rate=sr, autoplay=False) . . Your browser does not support the audio element. Sound signal as a time-series . On listening using the above widget you would have recognized the sound as repeated coughing. Lets plot the signal. . #collapse_show # plot fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,1,1) ax.plot(times, x) ax.grid(True) plt.ylabel(&#39;amplitude [in A.U.]&#39;, fontsize=14) plt.xlabel(&#39;time [in sec]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . The signal starts with silence, and around 0.5 sec we see the start of the first cough. This is followed by two more, and then a pause which might be an inhaling of air. Subsequently. we see three more coughs. Lets visualiza the distribution of the sample values. . #collapse_show fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,2,1) ax.hist(x,bins=1000,range=(x.min(), x.max())) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xlabel(&#39;amplitude [in A.U]&#39;, fontsize=14) # plt.xticks(fontsize=13) # plt.yticks(fontsize=13) plt.xlim(-.1,.1) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.scatter(x[:-1],x[1:]) ax.grid(True) plt.ylabel(&#39;x [n+1]&#39;, fontsize=14) plt.xlabel(&#39;x [n]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . We see that the histogram (shown on the left) peaks around zero. This means most of the sample values are low amplitude (&lt;.0125). We also plot a phase plot of x[n+1] vs x[n] on the right. This lies along y=x indicating a correlation between consecutive time samples. Such high correlation also implies high low frequency (relative to 8 kHz) content in the signal. We will verify this observation using Fourier transform. . Spectrum of a sound signal . A time-domain signal can be analyzed using Fourier transform. (To be fair, any signal can be analyzed using Fourier transform). Via a Fourier tranform we can visualize the frequency (or spectral) content in the signal. This is useful for sound signal analysis. The obtained spectral content can help us understand certain perceived attributes of the sound (namely, timbre). Lets compute and visualize the spectrum of the above sound signal. . #collapse_show def nearestpow2(n): k=1 while n&gt;2**k: k = k+1 return 2**k nfft = nearestpow2(len(x)) X = np.fft.rfft(x,nfft) freq = np.arange(0,nfft/2+1)/nfft*fs #collapse_show fig = plt.subplots(figsize=(16,5)) ax = plt.subplot(1,2,1) ax.plot(freq,np.abs(X)) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in A.U]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.plot(freq,20*np.log10(np.abs(X))-np.max(20*np.log10(np.abs(X)))) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in dB]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.ylim(-60,10) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) . . The plot on the left depicts the spectrum distributed from 0 to 8 kHz (=fs/2). We see a peak (like of a mountain) around 300 Hz, and a second peak around 1500 Hz, follwed by three more peaks. Also, there is a roll-off of the spectral amplitude from lower to higher frequencies. To decreases the contrast between the too high peak and other smaller peaks we can apply a dB transformation to the spectrum amplitude. The resulting plot is shown on the right. Applying dB transform for visualizing the spectral content also makes sense from perception aspects (you might have noticed sound pressure is reported in dBs). . Spectrograms . The Fourier tranform helps us understand the frequency content (spectrum) of this sound signal. This is useful. Does the ear also use a similar approach to analyze a sound signal? Scientists have dissected mammaliann ears and found that the organ inside the ear (referred as the cochlea) acts to certain extent like a mechanical Fourier analyzer. In relation do check out this video. But definitely the cochlea will not wait for the whole signal to end (like in our case 3.5 secs) and then compute the Fourier transform. We hear the sound within 10 msecs. While you are listening to speech, you don&#39;t wait for the person to finish to understand the speech, innstead you start understanding it while the person is speaking. Thus, lets take Fourier transform of small segments of the sound signal. But how small? You are free to choose any length. . We will use something in between 10-30 msec and the reasoning is that if the minumum frequency in the sound signal is 50 Hz then we would have atleast captured 2 cycles. | This also relates to non-stationarity in the signal. Sound signal such as speech and music have a time-varying spectral content. Hence, it makes sense to analyze the signal in short-time segments. | . Don&#39;t worry if the above is not clear to you. Below is an illustration on the same. Lets underatand this. From the speech signal you take a 25 msec segment, compute its magnitude Fourier transform, and push the the output into a column of an empty matrix. Then you hop in time by 10 msec, and again take a 25 msec segment from the speech signal, and repeat the same thing, that is, compute magnitude Fourier transform and push the output into the next columnn of the matrix. This way you move from the start to end of the signal by hopping in 10 msecs. The matrix you thus obtained is called the spectrogram. It is plotted as an image (color bar: the gradient from dark blue to white indicates high to low amplitude in the spectral content). Do you like this image? You can notice some beutiful horizontal striations. These correspond to harmonic frequencies in certain time-segments in the speech signal. Some amazing folks can read out a sentence by just starring at the spectrogram. Check out this for more details. . Now that we know about spectrogram, lets compute this for our cough signal. . #collapse_show # first we define the spectrogram function def generate_spectrogram(x,fs,wdur=20e-3,hdur=5e-3): X = [] i = 0 cnt = 0 win = np.hamming(wdur*fs) win = win - np.min(win) win = win/np.max(win) while i&lt;(len(x)-int(wdur*fs)): X.append(np.multiply(win,x[i:(i+int(wdur*fs))])) i = i + int(hdur*fs) cnt= cnt+1 X = np.array(X) Xs = abs(np.fft.rfft(X)) return Xs # lets plot now fig = plt.subplots(figsize=(6,1)) ax = plt.subplot(1,1,1) ax.plot(times,x) ax.set_xlim(times[0],times[-1]) ax.set_ylim(-1,1) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U&#39;) sns.despine(offset = .1,trim=False) plt.show() fig, ax = plt.subplots(figsize=(6,4)) Xs = generate_spectrogram(x,fs,wdur=10e-3,hdur=2.5e-3) XdB = 20*np.log10(Xs.T) XdB = XdB - np.max(XdB) im = ax.imshow(XdB,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,extent = [times[0], times[-1], 0, fs/2/1e3], cmap=&#39;RdBu_r&#39;,vmin = 0, vmax =-100) divider = make_axes_locatable(ax) colorbar_ax = fig.add_axes([.95, 0.1, 0.015, 0.5]) fig.colorbar(im, cax=colorbar_ax) ax.set_xlim(times[0],times[-1]) # ax.set_xlim(.2,3) ax.set_ylim(-.1,4) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;FREQ [in kHz]&#39;) sns.despine(offset = 0.01,trim=False) plt.show() . .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/2020/08/16/sound_visualization.html",
            "relUrl": "/coswara/tutorial/2020/08/16/sound_visualization.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Metadata visualization",
            "content": "The coswara database is created by crowdsourcing respiratory sound samples. In this post we will visualize the crowd (or participant) distribution along certain dimensions collected in metadata questionaire. We will read the CSV file containing the metadata information of all the users (as on 07 August 2020). From the whole dataset, we have manually listened to 941 participants&#39; audio samples. Below we present this data. A more detailed documentation is also available here and will be presented at the Interspeech 2020 conference. . First we visualize the gender, age, and country-wise (India/outside) distribution. . #collapse_show # import some packages import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) # load CSV file fname = &#39;combined_plus_annotated_IS2020.csv&#39; DF = pd.read_csv(&#39;./my_data/&#39;+fname) # plot gender information gender_labels = DF[&#39;g&#39;].unique() gender_cnt = [] for i in range(len(gender_labels)): gender_cnt.append(len(DF[(DF[&#39;g&#39;] == gender_labels[i]) &amp; DF[&#39;cough-heavy-quality&#39;]])) fig = plt.subplots(figsize=(16, 4)) ax = plt.subplot(1,3,1) ax.bar(2,gender_cnt[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) ax.bar(4,gender_cnt[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot;//&quot;,color=&#39;blue&#39;,width=.5) for i, v in enumerate(gender_cnt): ax.text(2*(i+1)-.2,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) plt.xticks([2,4], [&#39;MALE&#39;,&#39;FEMALE&#39;],rotation=0) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) # ax.set_xlim(0,5) # ax.set_ylim(200,1500) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plot age information age_labels = DF[&#39;a&#39;].unique() age_cnt = [] for i in range(len(age_labels)): age_cnt.append(len(DF[(DF[&#39;a&#39;] == age_labels[i])])) ax = plt.subplot(1,3,2) ax.bar(age_labels,age_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,color=&#39;blue&#39;) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xlabel(&#39;AGE&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plot country information country_labels = DF[&#39;l_c&#39;].unique() country_cnt = [] for i in range(len(country_labels)): country_cnt.append(len(DF[DF[&#39;l_c&#39;] == country_labels[i]])) country_cnt = np.array(country_cnt) indx = np.argsort(country_cnt)[::-1] country_cnt = country_cnt[indx] country_labels = country_labels[indx] two_categories = [country_cnt[0],np.sum(country_cnt[1:])] two_labels = [&#39;INDIA&#39;,&#39;OTHERS&#39;] ax = plt.subplot(1,3,3) ax.bar(2,two_categories[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) ax.bar(4,two_categories[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) plt.xticks([2,4],two_labels,rotation=0) for i, v in enumerate(two_categories): ax.text(2*(i+1)-.25,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . India has 29 states. Next, lets visualize the across state distribution. We show only top 5 below. . #collapse_show state_labels = DF[&#39;l_s&#39;].unique() state_cnt = [] for i in range(len(state_labels)): state_cnt.append(len(DF[DF[&#39;l_s&#39;] == state_labels[i]])) state_cnt = np.array(state_cnt) indx = np.argsort(state_cnt)[::-1][0:6] state_cnt = state_cnt[indx] state_labels = state_labels[indx] fig, ax = plt.subplots(figsize=(8, 4)) ax.bar(np.arange(0,len(state_cnt)),state_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.25) ax.set_ylabel(&#39;PARTICIPANT COUNT&#39;,fontsize=14) # # ax.text(1.5,-9,&#39;MEAN&#39;,horizontalalignment=&#39;center&#39;) plt.xticks(np.arange(0,len(state_cnt)),state_labels,rotation=30,fontsize=13) for i, v in enumerate(state_cnt): ax.text(i-.15,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . Next, we group the participants into two groups, healthy and unhealthy. This grouping is based on absence/presence of any respiratory ailments reported in the metadata questionaire. . #collapse_show labels = [&#39;HEALTHY&#39;,&#39;UNHEALTHY&#39;] category_cnt = [] category_cnt.append(len(DF.loc[(DF[&#39;covid_status&#39;]==&#39;healthy&#39;) &amp; (DF[&#39;asthma&#39;]!=True)&amp; (DF[&#39;cld&#39;]!=True)&amp; (DF[&#39;cold&#39;]!=True)&amp; (DF[&#39;cough&#39;]!=True)&amp; (DF[&#39;pneumonia&#39;]!=True)&amp; (DF[&#39;fever&#39;]!=True)])) category_cnt.append(len(DF.loc[(DF[&#39;covid_status&#39;]==&#39;resp_illness_not_identified&#39;) | (DF[&#39;covid_status&#39;]==&#39;positive_mild&#39;) | (DF[&#39;asthma&#39;]==True)| (DF[&#39;cld&#39;]==True)| (DF[&#39;cold&#39;]==True)| (DF[&#39;cough&#39;]==True)| (DF[&#39;pneumonia&#39;]==True)| (DF[&#39;fever&#39;]==True)])) fig = plt.subplots(figsize=(4,4)) ax = plt.subplot(1,1,1) ax.bar(2,category_cnt[0],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5,label=&#39;clean&#39;) ax.bar(4,category_cnt[1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5,label=&#39;noisy&#39;) plt.xticks([2,4],labels,rotation=0) for i, v in enumerate(category_cnt): ax.text(2*(i+1)-.25,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() plt.show() . . Lastly, we have listened to all the 941 participants&#39; audio data and classified every audio file into clean, noisy, and bad (highly degraded). The resulting distribution across the nine sound categories is shown below. . #collapse_show labels = [&#39;vowel-o&#39;,&#39;vowel-e&#39;,&#39;vowel-a&#39;,&#39;cough-shallow&#39;,&#39;cough-heavy&#39;,&#39;breathing-shallow&#39;,&#39;breathing-deep&#39;, &#39;counting-normal&#39;,&#39;counting-fast&#39;] category_cnt = [] for label in labels: category_cnt.append(len(DF[(DF[label]==label) &amp; ((DF[label+&#39;-quality&#39;]==&#39;clean_audio&#39;)) &amp; ((DF[label+&#39;-cont&#39;]==&#39;y&#39;)) &amp; (DF[label+&#39;-vol&#39;]==&#39;y&#39;)])) category_cnt.append(len(DF[(DF[label]==label) &amp; ((DF[label+&#39;-quality&#39;]==&#39;noisy_audio&#39;)) &amp; ((DF[label+&#39;-cont&#39;]==&#39;y&#39;))])) category_cnt.append(len(DF[(DF[label]==label) &amp; (((DF[label+&#39;-quality&#39;]==&#39;bad_audio&#39;)) |((DF[label+&#39;-quality&#39;]==&#39;clean_audio&#39;)&amp;(DF[label+&#39;-cont&#39;]==&#39;n&#39;)) |((DF[label+&#39;-quality&#39;]==&#39;noisy_audio&#39;)&amp;(DF[label+&#39;-cont&#39;]==&#39;n&#39;)) )])) fig = plt.subplots(figsize=(10,4)) ax = plt.subplot(1,1,1) cnt = 0 indx = 0 xticks = [] for i in range(len(category_cnt)//3): if i ==0: ax.bar(cnt,category_cnt[indx],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=1,label=&#39;clean&#39;) ax.bar(cnt+1,category_cnt[indx+1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,width=1,label=&#39;noisy&#39;) ax.bar(cnt+2,category_cnt[indx+2],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;green&#39;,width=1,label=&#39;bad&#39;) else: ax.bar(cnt,category_cnt[indx],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=1) ax.bar(cnt+1,category_cnt[indx+1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,width=1) ax.bar(cnt+2,category_cnt[indx+2],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;green&#39;,width=1) xticks.append(cnt+.5) cnt = cnt+4 indx = indx+3 ax.set_xticks(xticks) ax.set_xticklabels(labels,rotation=30,fontsize=13) ax.grid(True) ax.set_xlim(-2,cnt+2) ax.legend(loc=&#39;upper right&#39;,frameon=False,bbox_to_anchor=(1.05,1),fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.yticks(fontsize=13) plt.show() . . In another post we will attempt to describe the acoustic features of the audio samples. Looking forward to have you with us in in this exploration. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/15/visualize_metadata.html",
            "relUrl": "/coswara/2020/08/15/visualize_metadata.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Speech signals - a wonder",
            "content": "Some context . &quot;Tring, tring&quot;,... your phone is ringing. You pick it, and hear - &quot;Hello, what&#39;s up?&quot;. Often, this sound, pouring into your ears, will inform you about the gender of the talker, the identity, and the emotion. Isn&#39;t this fascinating? With only 2-3 secs of sound signal, the human brain has estimated so much about the talker. As the converstation progresses, you will be able to even estimate the personality, and the health status of the talker. Speech signal is immensely rich in information, and our brain is trained, since our childhood, to extract a lot from it. . Speech machinery . Let&#39;s see how this fascinating sound signal -speech, is produced. You inhale a breath, the air enters your lungs and creates a high pressure. When you speak, a sequence of co-ordinated mechanical processes are initiated, flexing the vocal tract muscles, and the result is the release of air pressure from mouth and nose. If you don&#39;t believe this - just pause - take a deep breathe, and read aloud the previous line. Didn&#39;t you exhale while reading it! The co-ordinated mechanical processes involved during speaking are a miracle. Just to give a context, human beings are the only species on earth which can produce the diverse range of acoustic sounds making up our the vocal communication repository. Biology suggests that the reason is linked to the FOXP2 gene - found only in humans. Every human is unique, and this uniqueness also reflects in the speech signals which allows us to easily recognize the voice of many. . Recording speech . Satisfying the human curisoity to record speech signals was a challenge. How do you store the speech sounds? Speak into a jar, close the lid, and open it, and bingo, you hear it! Sorry, physics won&#39;t allow this happen. In late 19th century, phonograph was invented, a beutiful mind behind this was Thomas Edison. As time progressed, this became popular to record music, and lead to gramophone, magentic tape cassetes, and compact discs. Now we just store it in solid state semiconductor drives without worrying about how! Technology has been a blessing when you consider the seamless manner in which we capture, store, process, and playback speech, music, and images. . Processing speech . Cool! we are able to record sound signals. Let&#39;s move on to processing sound signals. Can we design machine systems which extract information from sound signals? For instance, can machines perform speech recognition, speaker recognition, emotion recognition, and ... the list can go on. Our brains does all this. It shouldn&#39;t be impossible to design machines to do this, and beyond this too! Well, automatic speech recognition is now a reality, and accessible in our mobile phones/laptops. Challenges exist when the speech recording is noisy, accented, not in English, or has multiple talkers etc. For single talker, clean recording, American/British accent, English speech, the machine systems work quite well. Similar is the performance for talker recognition as well. What is the key behind this technology? Welcome to the world of data analysis and machine learning! . Enters signal processing . In the follow-up posts we will discuss about to process and analyze speech signals using python. Looking forward to have you in this expedition! .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/06/20/speech_signals.html",
            "relUrl": "/coswara/2020/06/20/speech_signals.html",
            "date": " • Jun 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This blog shares our journey in the Coswara project. The project is launched by the LEAP lab, based at the Indian Institute of Science, Bangalore. .",
          "url": "https://iiscleap.github.io/coswara-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://iiscleap.github.io/coswara-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}