{
  
    
        "post0": {
            "title": "Feature Extraction",
            "content": "With the advent of well engineered sensors, the practice of data collection has gained pace. Individuals and groups have put significant efforts to record and host hours of audio recordings on the internet. Also, many of these are accessible at the click of a mouse. Is all data also information? Information is always for a purpose, and hence, the proper question will be - is this data informative for so and so task? To answer this we have to go beyond just visualizing sound signals, and understand the concept of features. . Example:How do you distinguish tomatoes from potatoes, without tasting? You will likely use characteristics such as color (red vs brown), shape (close to spherical vs somewhat spherical), tightness (soft vs hard), etc. What we listed are the features of vegetables, and values these features take can help us distinguish one vegetable from other. How? In the figure shown below we show a cartoon schematic of intensity of reflected light by a 100 potatoes and tomatoes. We can see that a majority of tomatoes reflect more light than potato and hence, this can serve as a feature to distinguish one from the other. We also see that around an intensity of 102 the distinction is not very clear. . #collapse import numpy as np import matplotlib.pyplot as plt import seaborn as sns np.random.seed(seed=10) x = [] x.append(np.random.randn(100)+100) x.append(2*np.random.randn(100)+105) # make plot fig = plt.subplots(figsize=(8,4)) ax = plt.subplot(1,1,1) sns.distplot(x[0],color=&#39;r&#39;,label=&#39;potato&#39;) sns.distplot(x[1],color=&#39;b&#39;,label=&#39;tomato&#39;) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.legend(frameon=False) plt.xlabel(&#39;(May be) intensity of reflected light [in A.U.]&#39;) plt.ylabel(&#39;DENSITY&#39;) plt.show() . . Features of sound signals . What features can help distinguish one kind of sound from another? We can treat a sound signal (time-series) as an object (like we did for tomatoes and potatoes) and consider features such as average amplitude, variance in amplitude values, duration of silence, etc. A cartoon illustration is shown in the below figure. . . To be honest, such simple file level feature won&#39;t help in distinguising different sound types. Can we be little more clever? Yes, and we can ask the following questions: . Do we know how our ear process the sound signals? We know a lot about this from the filed of physiology. Infact, contributions towards answering this has also been awarded a Noble Prize (Georg von Békésy, 1961). It seems our hearing system housed inside the ear does a spectral analysis of the pressure variations. This spectral information is encoded in neural firinngs and sent to the higher auditory pathways, including the brain. Instead of writing a lot I will suggest you see this video here. | Do we know how our brain distinguish a dog barking from a tiger roaring? The answer to this is being actively studied in the field of psychoacoustic (how does mind process sound). We know from several studies that the spectral information is critical for perception. We know a lot more about speech signals. For example, we know that pitch, loudness, and timbre are critical in distinguising one voice from another. | . Based on these observations researchers have designed methods to estimate numerous features from sound signals. Two python packages we will use are Librosa and Parselmouth. A key aspect which is quite widely popular in sound signal analysis is to estimate features from short-time segments of the signal (instead of the file level features depicted in the previous figure). These short-time segments are usually 25 msec in duration (Why?). The below figure provides an illustration. . Let&#39;s write some code to extract the intensity of sound computed over short-time segments from a cough sound signal. From the plot shown below you can see that intensity is a single nummber, and it is high at instants of coughs. . #collapse import parselmouth import librosa from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable sr = 16000 fname = &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39; dname = &#39;./my_data/&#39; # for PRAAT hop_dur = .01 num_form = 3 max_form_freq = 5500 STANDARD_INTENSITY = 70. # call parselmouth to load sound snd = parselmouth.Sound(dname+fname) snd.scale_intensity(STANDARD_INTENSITY) x = snd.values[0] sr = snd.sampling_frequency times = np.arange(0,len(x),1)/sr # estimate intensity and spectrogram intensity = snd.to_intensity(minimum_pitch = 200.0, time_step=hop_dur,subtract_mean=False).values[0] spectrogram = snd.to_spectrogram(window_length=0.04) fig = plt.subplots(figsize=(12,12)) ax = plt.subplot(3,1,1) ax.plot(times,x) ax.set_xlabel(&quot;TIME [in s]&quot;) ax.set_ylabel(&#39;AMPLITUDE[A.U.]&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.xlim(times[0],times[-1]) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(3,1,2) ax.plot(np.arange(0,len(intensity),1)*hop_dur,intensity) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;intensity [in dB]&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.xlim(times[0],times[-1]) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fig,ax = plt.subplots(1,1,figsize=(12,5)) ax = plt.subplot(3,1,3) dynamic_range = 70 X, Y = spectrogram.x_grid(), spectrogram.y_grid() sg_db = 10 * np.log10(spectrogram.values) im = ax.pcolormesh(X, Y, sg_db, vmin=sg_db.max() - dynamic_range, cmap=&#39;RdBu_r&#39;) plt.ylim([spectrogram.ymin, spectrogram.ymax]) plt.xlabel(&quot;TIME [in s]&quot;) plt.ylabel(&quot;FREQUENCY [in Hz]&quot;) plt.show() . . Acoustic Features from Coswara Database . Lets visualize some acoustic features of sound samples from the COSWARA database. We will visualize different features via grouping of sound identity and gender. In this notebook we will focus on the sustained phonation (vowel) sound samples. The existing literature in vowel phonetics will help us interpret the estimated feature values. The different acoustic features are: . intensity | pitch or fundamental frequency | formant frequencies | harmonic-to-noise ratio (HNR) | . Extraction procedure . Given a sound sample (or here, an audio file), we first extract these for every 25 msec short-time segment obtained at hops of 10 msec. We used the Parselmouth python package (which uses Praat within it) for this. As an example, you can see the intensity feature vector plotted in the above plot. You can see that regions of low intensity correspond to (close to) silence regions in the sound signals. We use this observation to select short-time segments corresponding to voice activity. You can guess that intensity can be high even for a loud background noise, not coming from the vocal tract. Hence, together with intensity, we also use pitch and HNR values to decide voice activity regions. Finally, we will compute the average of each of these features over all voice-activity detected short-time segments. . File duration before and after VAD . The figure below shows the file duration of the sound samples from close to 650 individuals. Here we have considered only the clean audio files (annotated by human listeners, described in the Metadata post). In the scatter plot you can also see the reduced file duration post VAD. The thresholds for intensity, pitch and HNR are mentioned in the figure and choosen experimentally after observations on a set of files and knowledge from phonetics. Every sound file has some voiceless activity hence, the data points lie below y=x line. We can see that some sound files have a duration greater than 25 sec!, and this is true even after VAD. We also confirmed this by listening to these files, some individuals are really amazing at sustaining there phonation. . . Distribution of Pitch values . The signal processing quantification of perceived pitch of a voice signal is obtained by computing the fundamental frequency of the assumed harmonic complex in the signal. The origin of this hamronic complex in the voice signal owes to the vibrations (opening and closing) of the vocal folds in the glottis (shown in the figure below). Below we show the average pitch distribution obtainned by pooling all the sound sample files (each file belongs to a different individual). There is a clear distinction in the peaks of the distribution for the two gender (male/female). Males have a lower pitch than females (to know more on why, see our post on What&#39;s inside the coswara database). The distribution is not very different for the three vowels. This is expected from the insights on voice production. Pitch is more dependent on vibration of glottis and less dependent on the vocal tract configurations. . . Distribution of Formant values . We tracked the first two formants for every sound file. In the plot below we depict the distribution of average values of this obtained by pooling the features from all the sound files. All the distributions are unimodal. These distributions are somewhat different w.r.t. peak location for the three vowels. This is because of the distinct vocal tract configurations associated with each of the vowels. . Distribution of HNR values . The average harmonic-to-noise ratio (HNR) can help as quantify the clarity of voicing in each file. A HNR = 0 dB will indicate both noise and speech have equal energy. A HNR&gt;20 dB indicates a good voicing, the higher the better. Hoarse voice has lower HNR (&lt; 20 dB) . Although not shown here, HNR is sensitive to vowel kind. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/2020/08/17/feature_extraction.html",
            "relUrl": "/coswara/tutorial/2020/08/17/feature_extraction.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Coswara Database_a closer look",
            "content": "About the database . We have released a web application for data collection via crowsourcing. Essentially, this way anyone with a mobile phone and internet connectivity can contribute to the dataset. The website link can be accessed here. We target particpants from following three populations: . healthy: these are individuals with no respiratory illness | unhealthy: these are individuals with respiratory illness | COVID-19 positive: these are individuals identified as COVID-19 positive after RT-PCR test | . Metadata description . When a user opens the website, s/he is asked to fill a short questionaire which helps us collect metadata to categorize the user into one of the above three population. The complete metadata is composed of age, gender, location (country, state/province), current health status (healthy / exposed / cured / infected) and the presence of co-morbidity (pre-existing medical conditions) information. We do not collect any personally identifiable information. Each user gets a unique anonymized ID during data storage. A screenshot of the coswara webpage is provided below. Here, page 1 and 2 corresponnd to the metadata collection. . . Sound sample description . In the screenshot shown above, page 3 corresponds to the &quot;audio sample collection&quot; step. We collect audio samples corresponding to nine categories shown in the figure below. These categories as chosen to capture sound signals which embed in them most of the attributes of the respiratory system associated with speech production. To understand how let&#39;s tae a small detour to speech production system. . Art of speaking . The human speech production system draws contributions from diaphragm, lungs, trachea, larynx, pharynx, tounge, nasal cavity, and lips. You may note that many of these organs are not solely dedicated for speech production, example, we use mouth for eating and lungs to purify the air! Speech and vocal sound production is is an extra feat achieved, thanks to evolution, by these organs. . Lungs have elastic property, as they are in some sense repurposed swim bladders. Duing normal respiration, the diaphragm and the abdominal muscles between the ribs work together to expand the lungs. The elastic recoil of the lungs then provides the force that expels air during expiration. This means that the alveolar air pressure increases when you inhale and decreases when you exhale. Something different happens when you speak. . Speaking happens during exhaling. The aleveolar air pressure is released gradually in a co-ordinated manner via the opening and closing of the vocal cords (in the glottis). Something interesting happens here. For voiced sounds, such as vowels, the vocal folds open and close in a periodic fashion. This rate of opening and closing results in imparting periodicity to the output sound pressure wave. Further, this periodicity is one of the easiest perceived attributes in speech, and is referred to as the pitch of the speaker. You would have noticed that male speakers usually have lower pitch than female speakers, and female speakers have lower pitch than kid speakers. Why so? This is related to mass of the vocal folds. Heavier mass means lower pitch, and male anatomy often reveals a higher mass of the vocal folds. But that does not you cannot change your pitch. You can by altering the tension of the vocal folds, and we often do this when we want to emphasize something in our speech. Another attribute of speech we perceive quite easily is loudness. Increase in airflow from the lungs blows the vocal folds wider apart resulting in increased strength of the output pressure wave, thus making the sound louder. Voiced sounds are just one category of speech sounds. For unvoiced sounds, such as fricatives, the vocal folds remain open, and for stop consonant sounds, the vocal folds remain closed. Note that, these sounds do not have any perceived pitch associated with them. The below figure shows a schematic of human speech production system. . . What happens during coughing? The textbook explaination suggests that cough is a reflex action. The diaphragm contract, creating a negative pressure around the lung, and the glottis opens. This enables air to rush into the lungs in order to equalise the pressure. The glottis closes and the vocal cords contract to shut the glottis. The abdominal muscles contract to accentuate the action of the relaxing diaphragm, simultaneously, the other expiratory muscles contract. These actions increase the pressure of air within the lungs. The vocal cords relax and the glottis opens, releasing air at over 100 mph. The bronchi and non-cartilaginous portions of the trachea collapse to form slits through which the air is forced, which clears out any irritants attached to the respiratory lining. So a single cough will have no periodic opening and closing of vocal folds, unlike in vowels. However, often natural coughing results in a sequence of 3-4 coughs, and the physiological description of the opening and closing of glottis can become difficult to describe. An attempt to understand this is made here . What happens during breathing? The glottis largely remains open to enhance free flow of air into and out from the lungs, co-ordinated by the movement of the diaphragm and elasticity of the lungs. A nice video is shown here. . Why nine sound categories . As discussed above, we ask every user to record and upload nine sound samples. These can be grouped as follows: . breathing (two kinds; shallow and deep) | coughing (two kinds; shallow and heavy) | . The choice of the above two is is driven by the reporting by WHO and CDC which have listed dry cough, difficulty in breathing, and chest pain (or pressure) as key symptoms of this viral infection, visible between 2-14 days after exposure to the virus. Also, a recent modeling study of symptoms data collected from a pool of 7178 COVID-19 positive individuals validated the presence of these symptoms, and proposed a real-time prediction and tracking approach. Repeated coughing can adversely impact the mass and tension in the vocal folds. This can in turn alter the speaking style of the patient. You might of noticed that you can make a guess if your friend has cold his/her speaking style over phone. . sustained vowel phonations (three kinds; /ey/~as in made, /i/~as in beet, /u:/ as in cool) | . The choosen vowels have special place in the quantal theory of speech. These vowels are easy to produce and appear alomost in every spoken language. Further, these vowel sounds are perceived as most distinct amongst all other vowels, and have been argued to capture the vocal tract attributes effectively. For more details see here and here. . one to twenty digit counting (two kinds; normal and fast paced) | . Counting a sequence of digits corresponds to continuous speaking for close to 20 secs. Any breathing difficulty will make this task difficult, and we expect this to reflect in the speaking style such as loudness, stress and pause patterns, and pace of speaking. . Visualizing the waveforms . In the figure below we show an illustration of the waveforms and the corresponding spectrograms of few sound samples. The waveforms represent the recorded time-domain signal. Here, the spectrogram depicts the spectral content of the signal in every 10 msec short-time window of the signal. We can make some observations from the shown plots. . . The breathing samples are wideband. The spectral energy is distributed over all frequecies. The inhale is lower in energy than the exhale however, both lasts for a similar time-spane, close to 1 sec. The exhale (the center burst) also depicts some formant like structure in the spectrogram. This can be expected as the air travels through the vocal tract. . | For the coughing samples, we can see that these are repeating, and the first cough is little longer in duration. This can often happen as usually we take a deep breathe and release more in the first cough. Also, we can now see some formant structure also in the spectrogram. . | For sustained vowel phonations, we can see clear distinct formant structure, specifically, for the second formant in the spectrogram. . | For the digit counting, we can see the fluctuating formant structure in the spectrogram. . | . It should be noted that these recordings are obtained via crowdsourcing and recorded through web browsers. All sound samples are recorded at 48 kHz in WAV file format. Some of these recording may have ambient noise which cannot be filtered while recording. In another post we will try quantifying different artifacts we observe in these files. We manually listen to every uploaded file, and will share are opinion on the quality and curation procedure. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/17/closerlook.html",
            "relUrl": "/coswara/2020/08/17/closerlook.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Visualizing sound signals",
            "content": "Before the digital revolution, which started in 1960s, sound was predominantly recorded by etching the pressure waveform on a physical medium. During playback the etched waveform was tracked and converted to vibrations of a diaphragm. This is a beautiful idea, and you will be amazed to know that the quality was very good. To know more try seeing this video. . In current times, having witnessed the digital revolution, sound signals are stored as discrete sequence of numbers in in physical mediums such semiconductors. This approch is more efficient compared to vinyl records, and hence, have made the capture and playback of sound signals easily accessible. Your mobile phone does it everytime you are talking on phone. When it comes to digital capture of an analogue signal, following needs some understanding, . sampling frequency (fs): As the name implies, it refers to picking few samples from a continuous-time signal (or waveform). Sampling frequency refers to the number of samples you pick per second, samples being spaced uniformly apart in time. Obviously, you are while you are sampling you are also discarding a lot. Is there a critical sampling frequency when the discarding is not going to hurt you? Yes, and this rate is referred to as the Nyquist-rate. It is equal to the twice the maximum frequency content in the continuous-time signal you are interested to sample. In our case this signal of interest is sound, and a good choice of fs is 48 kHz for music signals and 16 kHz for speech signals. A beautiful illustration of this is provided here and more details here . | quantization: Once we have sampled along time, the next step is to store the amplitude values taken by the signal at the sampled time instants. These ampltude values will be stored in the computer (or more specifically, disk drives) and this storage has a finite resolution for storing a number decided by the number of bits used to represent a number. These bits can be n = 2,4,8, 16, 32, etc. For n bits, the resolution is (1/(2^n-1)). So, more the number of bits, higher is the resolution and hence, the representation (or storage) of the number in the memory of the computer will be more accurate. Sound (or audio) is usually stored at 16-bits. For more details you can read this). . | . Hurray, a sampled and quantized analogue signal can be stored in any digital media. And once we have stored it, we can also read (or load) it back from the digital media! In step 1, we will attempt reading (or loading) a sound file stored in the github server, and visualize (and hear) the content inside it. . Loading a sound file . We will load a file &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39;. Any WAV file has some metadata stored inside it. This metadata gives information about the sampling frequency, quantization bits, number of channel, etc., used during the capture (and) storage of the sound file. Below we show a screenshot of this information for our file. . . We can see that the fs is 16 kHz. Lets now load, listen, and plot the data inside this sound file. . #collapse import numpy as np import librosa from IPython.lib.display import Audio import matplotlib.pyplot as plt from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable import seaborn as sns fs = 16000 fname = &#39;nMIOAh7qRFf3pqbchclOLKbPDOm1_heavy_cough.wav&#39; dname = &#39;./my_data/&#39; # load x, sr = librosa.load(dname+fname,sr=16000) x = x/max(np.abs(x)) times = np.arange(0,len(x))/fs # listen Audio(x, rate=sr, autoplay=False) . . Your browser does not support the audio element. Sound signal as a time-series . On listening using the above widget you would have recognized the sound as repeated coughing. Lets plot the signal. . #collapse # plot fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,1,1) ax.plot(times, x) ax.grid(True) plt.ylabel(&#39;amplitude [in A.U.]&#39;, fontsize=14) plt.xlabel(&#39;time [in sec]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . The signal starts with silence, and around 0.5 sec we see the start of the first cough. This is followed by two more, and then a pause which might be an inhaling of air. Subsequently. we see three more coughs. Lets visualiza the distribution of the sample values. . #collapse_show fig = plt.subplots(figsize=(12,4)) ax = plt.subplot(1,2,1) ax.hist(x,bins=1000,range=(x.min(), x.max())) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xlabel(&#39;amplitude [in A.U]&#39;, fontsize=14) # plt.xticks(fontsize=13) # plt.yticks(fontsize=13) plt.xlim(-.1,.1) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.scatter(x[:-1],x[1:]) ax.grid(True) plt.ylabel(&#39;x [n+1]&#39;, fontsize=14) plt.xlabel(&#39;x [n]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . We see that the histogram (shown on the left) peaks around zero. This means most of the sample values are low amplitude (&lt;.0125). We also plot a phase plot of x[n+1] vs x[n] on the right. This lies along y=x indicating a correlation between consecutive time samples. Such high correlation also implies high low frequency (relative to 8 kHz) content in the signal. We will verify this observation using Fourier transform. . Spectrum of a sound signal . A time-domain signal can be analyzed using Fourier transform. (To be fair, any signal can be analyzed using Fourier transform). Via a Fourier tranform we can visualize the frequency (or spectral) content in the signal. This is useful for sound signal analysis. The obtained spectral content can help us understand certain perceived attributes of the sound (namely, timbre). Lets compute and visualize the spectrum of the above sound signal. . #collapse def nearestpow2(n): k=1 while n&gt;2**k: k = k+1 return 2**k nfft = nearestpow2(len(x)) X = np.fft.rfft(x,nfft) freq = np.arange(0,nfft/2+1)/nfft*fs #collapse_show fig = plt.subplots(figsize=(16,5)) ax = plt.subplot(1,2,1) ax.plot(freq,np.abs(X)) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in A.U]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(1,2,2) ax.plot(freq,20*np.log10(np.abs(X))-np.max(20*np.log10(np.abs(X)))) ax.grid(True) plt.ylabel(&#39;MAGNITUDE SPECTRUM [in dB]&#39;, fontsize=14) plt.xlabel(&#39;frequency [in Hz]&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.ylim(-60,10) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) . . The plot on the left depicts the spectrum distributed from 0 to 8 kHz (=fs/2). We see a peak (like of a mountain) around 300 Hz, and a second peak around 1500 Hz, follwed by three more peaks. Also, there is a roll-off of the spectral amplitude from lower to higher frequencies. To decreases the contrast between the too high peak and other smaller peaks we can apply a dB transformation to the spectrum amplitude. The resulting plot is shown on the right. Applying dB transform for visualizing the spectral content also makes sense from perception aspects (you might have noticed sound pressure is reported in dBs). . Spectrograms . The Fourier tranform helps us understand the frequency content (spectrum) of this sound signal. This is useful. Does the ear also use a similar approach to analyze a sound signal? Scientists have dissected mammaliann ears and found that the organ inside the ear (referred as the cochlea) acts to certain extent like a mechanical Fourier analyzer. In relation do check out this video. But definitely the cochlea will not wait for the whole signal to end (like in our case 3.5 secs) and then compute the Fourier transform. We hear the sound within 10 msecs. While you are listening to speech, you don&#39;t wait for the person to finish to understand the speech, innstead you start understanding it while the person is speaking. Thus, lets take Fourier transform of small segments of the sound signal. But how small? You are free to choose any length. . We will use something in between 10-30 msec and the reasoning is that if the minumum frequency in the sound signal is 50 Hz then we would have atleast captured 2 cycles. | This also relates to non-stationarity in the signal. Sound signal such as speech and music have a time-varying spectral content. Hence, it makes sense to analyze the signal in short-time segments. | . Don&#39;t worry if the above is not clear to you. Below is an illustration on the same. Lets underatand this. From the speech signal you take a 25 msec segment, compute its magnitude Fourier transform, and push the the output into a column of an empty matrix. Then you hop in time by 10 msec, and again take a 25 msec segment from the speech signal, and repeat the same thing, that is, compute magnitude Fourier transform and push the output into the next columnn of the matrix. This way you move from the start to end of the signal by hopping in 10 msecs. The matrix you thus obtained is called the spectrogram. It is plotted as an image (color bar: the gradient from dark blue to white indicates high to low amplitude in the spectral content). Do you like this image? You can notice some beutiful horizontal striations. These correspond to harmonic frequencies in certain time-segments in the speech signal. Some amazing folks can read out a sentence by just starring at the spectrogram. Check out this for more details. . Now that we know about spectrogram, lets compute this for our cough signal. . #collapse # first we define the spectrogram function def generate_spectrogram(x,fs,wdur=20e-3,hdur=5e-3): X = [] i = 0 cnt = 0 win = np.hamming(wdur*fs) win = win - np.min(win) win = win/np.max(win) while i&lt;(len(x)-int(wdur*fs)): X.append(np.multiply(win,x[i:(i+int(wdur*fs))])) i = i + int(hdur*fs) cnt= cnt+1 X = np.array(X) Xs = abs(np.fft.rfft(X)) return Xs # lets plot now fig = plt.subplots(figsize=(6,1)) ax = plt.subplot(1,1,1) ax.plot(times,x) ax.set_xlim(times[0],times[-1]) ax.set_ylim(-1,1) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U&#39;) sns.despine(offset = .1,trim=False) plt.show() fig, ax = plt.subplots(figsize=(6,4)) Xs = generate_spectrogram(x,fs,wdur=10e-3,hdur=2.5e-3) XdB = 20*np.log10(Xs.T) XdB = XdB - np.max(XdB) im = ax.imshow(XdB,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,extent = [times[0], times[-1], 0, fs/2/1e3], cmap=&#39;RdBu_r&#39;,vmin = 0, vmax =-100) divider = make_axes_locatable(ax) colorbar_ax = fig.add_axes([.95, 0.1, 0.015, 0.5]) fig.colorbar(im, cax=colorbar_ax) ax.set_xlim(times[0],times[-1]) # ax.set_xlim(.2,3) ax.set_ylim(-.1,4) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;FREQ [in kHz]&#39;) sns.despine(offset = 0.01,trim=False) plt.show() . .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/tutorial/2020/08/16/sound_visualization.html",
            "relUrl": "/coswara/tutorial/2020/08/16/sound_visualization.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Metadata visualization",
            "content": "The coswara database is created by crowdsourcing respiratory sound samples. In this post we will visualize the crowd (or participant) distribution along certain dimensions collected in metadata questionaire. We will read the CSV file containing the metadata information of all the users (as on 07 August 2020). From the whole dataset, we have manually listened to 941 participants&#39; audio samples. Below we present this data. A more detailed documentation is also available here and will be presented at the Interspeech 2020 conference. . First we visualize the gender, age, and country-wise (India/outside) distribution. . #collapse # import some packages import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) # load CSV file fname = &#39;combined_plus_annotated_IS2020.csv&#39; DF = pd.read_csv(&#39;./my_data/&#39;+fname) # plot gender information gender_labels = DF[&#39;g&#39;].unique() gender_cnt = [] for i in range(len(gender_labels)): gender_cnt.append(len(DF[(DF[&#39;g&#39;] == gender_labels[i]) &amp; DF[&#39;cough-heavy-quality&#39;]])) fig = plt.subplots(figsize=(16, 4)) ax = plt.subplot(1,3,1) ax.bar(2,gender_cnt[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) ax.bar(4,gender_cnt[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot;//&quot;,color=&#39;blue&#39;,width=.5) for i, v in enumerate(gender_cnt): ax.text(2*(i+1)-.2,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) plt.xticks([2,4], [&#39;MALE&#39;,&#39;FEMALE&#39;],rotation=0) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) # ax.set_xlim(0,5) # ax.set_ylim(200,1500) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plot age information age_labels = DF[&#39;a&#39;].unique() age_cnt = [] for i in range(len(age_labels)): age_cnt.append(len(DF[(DF[&#39;a&#39;] == age_labels[i])])) ax = plt.subplot(1,3,2) ax.bar(age_labels,age_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,color=&#39;blue&#39;) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xlabel(&#39;AGE&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plot country information country_labels = DF[&#39;l_c&#39;].unique() country_cnt = [] for i in range(len(country_labels)): country_cnt.append(len(DF[DF[&#39;l_c&#39;] == country_labels[i]])) country_cnt = np.array(country_cnt) indx = np.argsort(country_cnt)[::-1] country_cnt = country_cnt[indx] country_labels = country_labels[indx] two_categories = [country_cnt[0],np.sum(country_cnt[1:])] two_labels = [&#39;INDIA&#39;,&#39;OTHERS&#39;] ax = plt.subplot(1,3,3) ax.bar(2,two_categories[0], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) ax.bar(4,two_categories[1], align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5) plt.xticks([2,4],two_labels,rotation=0) for i, v in enumerate(two_categories): ax.text(2*(i+1)-.25,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) plt.ylabel(&#39;PARTICIPANT COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . India has 29 states. Next, lets visualize the across state distribution. We show only top 5 below. . #collapse state_labels = DF[&#39;l_s&#39;].unique() state_cnt = [] for i in range(len(state_labels)): state_cnt.append(len(DF[DF[&#39;l_s&#39;] == state_labels[i]])) state_cnt = np.array(state_cnt) indx = np.argsort(state_cnt)[::-1][0:6] state_cnt = state_cnt[indx] state_labels = state_labels[indx] fig, ax = plt.subplots(figsize=(8, 4)) ax.bar(np.arange(0,len(state_cnt)),state_cnt, align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.25) ax.set_ylabel(&#39;PARTICIPANT COUNT&#39;,fontsize=14) # # ax.text(1.5,-9,&#39;MEAN&#39;,horizontalalignment=&#39;center&#39;) plt.xticks(np.arange(0,len(state_cnt)),state_labels,rotation=30,fontsize=13) for i, v in enumerate(state_cnt): ax.text(i-.15,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . Next, we group the participants into two groups, healthy and unhealthy. This grouping is based on absence/presence of any respiratory ailments reported in the metadata questionaire. . #collapse labels = [&#39;HEALTHY&#39;,&#39;UNHEALTHY&#39;] category_cnt = [] category_cnt.append(len(DF.loc[(DF[&#39;covid_status&#39;]==&#39;healthy&#39;) &amp; (DF[&#39;asthma&#39;]!=True)&amp; (DF[&#39;cld&#39;]!=True)&amp; (DF[&#39;cold&#39;]!=True)&amp; (DF[&#39;cough&#39;]!=True)&amp; (DF[&#39;pneumonia&#39;]!=True)&amp; (DF[&#39;fever&#39;]!=True)])) category_cnt.append(len(DF.loc[(DF[&#39;covid_status&#39;]==&#39;resp_illness_not_identified&#39;) | (DF[&#39;covid_status&#39;]==&#39;positive_mild&#39;) | (DF[&#39;asthma&#39;]==True)| (DF[&#39;cld&#39;]==True)| (DF[&#39;cold&#39;]==True)| (DF[&#39;cough&#39;]==True)| (DF[&#39;pneumonia&#39;]==True)| (DF[&#39;fever&#39;]==True)])) fig = plt.subplots(figsize=(4,4)) ax = plt.subplot(1,1,1) ax.bar(2,category_cnt[0],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5,label=&#39;clean&#39;) ax.bar(4,category_cnt[1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=.5,label=&#39;noisy&#39;) plt.xticks([2,4],labels,rotation=0) for i, v in enumerate(category_cnt): ax.text(2*(i+1)-.25,v + 3, str(v), color=&#39;black&#39;, fontweight=&#39;bold&#39;,fontsize=14) ax.grid(True) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() plt.show() . . Lastly, we have listened to all the 941 participants&#39; audio data and classified every audio file into clean, noisy, and bad (highly degraded). The resulting distribution across the nine sound categories is shown below. . #collapse labels = [&#39;vowel-o&#39;,&#39;vowel-e&#39;,&#39;vowel-a&#39;,&#39;cough-shallow&#39;,&#39;cough-heavy&#39;,&#39;breathing-shallow&#39;,&#39;breathing-deep&#39;, &#39;counting-normal&#39;,&#39;counting-fast&#39;] category_cnt = [] for label in labels: category_cnt.append(len(DF[(DF[label]==label) &amp; ((DF[label+&#39;-quality&#39;]==&#39;clean_audio&#39;)) &amp; ((DF[label+&#39;-cont&#39;]==&#39;y&#39;)) &amp; (DF[label+&#39;-vol&#39;]==&#39;y&#39;)])) category_cnt.append(len(DF[(DF[label]==label) &amp; ((DF[label+&#39;-quality&#39;]==&#39;noisy_audio&#39;)) &amp; ((DF[label+&#39;-cont&#39;]==&#39;y&#39;))])) category_cnt.append(len(DF[(DF[label]==label) &amp; (((DF[label+&#39;-quality&#39;]==&#39;bad_audio&#39;)) |((DF[label+&#39;-quality&#39;]==&#39;clean_audio&#39;)&amp;(DF[label+&#39;-cont&#39;]==&#39;n&#39;)) |((DF[label+&#39;-quality&#39;]==&#39;noisy_audio&#39;)&amp;(DF[label+&#39;-cont&#39;]==&#39;n&#39;)) )])) fig = plt.subplots(figsize=(10,4)) ax = plt.subplot(1,1,1) cnt = 0 indx = 0 xticks = [] for i in range(len(category_cnt)//3): if i ==0: ax.bar(cnt,category_cnt[indx],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=1,label=&#39;clean&#39;) ax.bar(cnt+1,category_cnt[indx+1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,width=1,label=&#39;noisy&#39;) ax.bar(cnt+2,category_cnt[indx+2],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;green&#39;,width=1,label=&#39;bad&#39;) else: ax.bar(cnt,category_cnt[indx],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;blue&#39;,width=1) ax.bar(cnt+1,category_cnt[indx+1],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,width=1) ax.bar(cnt+2,category_cnt[indx+2],align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;green&#39;,width=1) xticks.append(cnt+.5) cnt = cnt+4 indx = indx+3 ax.set_xticks(xticks) ax.set_xticklabels(labels,rotation=30,fontsize=13) ax.grid(True) ax.set_xlim(-2,cnt+2) ax.legend(loc=&#39;upper right&#39;,frameon=False,bbox_to_anchor=(1.05,1),fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.ylabel(&#39;COUNT&#39;, fontsize=14) plt.yticks(fontsize=13) plt.show() . . In another post we will attempt to describe the acoustic features of the audio samples. Looking forward to have you with us in in this exploration. .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/08/15/visualize_metadata.html",
            "relUrl": "/coswara/2020/08/15/visualize_metadata.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Speech signals - a wonder",
            "content": "Some context . &quot;Tring, tring&quot;,... your phone is ringing. You pick it, and hear - &quot;Hello, what&#39;s up?&quot;. Often, this sound, pouring into your ears, will inform you about the gender of the talker, the identity, and the emotion. Isn&#39;t this fascinating? With only 2-3 secs of sound signal, the human brain has estimated so much about the talker. As the converstation progresses, you will be able to even estimate the personality, and the health status of the talker. Speech signal is immensely rich in information, and our brain is trained, since our childhood, to extract a lot from it. . Speech machinery . Let&#39;s see how this fascinating sound signal -speech, is produced. You inhale a breath, the air enters your lungs and creates a high pressure. When you speak, a sequence of co-ordinated mechanical processes are initiated, flexing the vocal tract muscles, and the result is the release of air pressure from mouth and nose. If you don&#39;t believe this - just pause - take a deep breathe, and read aloud the previous line. Didn&#39;t you exhale while reading it! The co-ordinated mechanical processes involved during speaking are a miracle. Just to give a context, human beings are the only species on earth which can produce the diverse range of acoustic sounds making up our the vocal communication repository. Biology suggests that the reason is linked to the FOXP2 gene - found only in humans. Every human is unique, and this uniqueness also reflects in the speech signals which allows us to easily recognize the voice of many. . Recording speech . Satisfying the human curisoity to record speech signals was a challenge. How do you store the speech sounds? Speak into a jar, close the lid, and open it, and bingo, you hear it! Sorry, physics won&#39;t allow this happen. In late 19th century, phonograph was invented, a beutiful mind behind this was Thomas Edison. As time progressed, this became popular to record music, and lead to gramophone, magentic tape cassetes, and compact discs. Now we just store it in solid state semiconductor drives without worrying about how! Technology has been a blessing when you consider the seamless manner in which we capture, store, process, and playback speech, music, and images. . Processing speech . Cool! we are able to record sound signals. Let&#39;s move on to processing sound signals. Can we design machine systems which extract information from sound signals? For instance, can machines perform speech recognition, speaker recognition, emotion recognition, and ... the list can go on. Our brains does all this. It shouldn&#39;t be impossible to design machines to do this, and beyond this too! Well, automatic speech recognition is now a reality, and accessible in our mobile phones/laptops. Challenges exist when the speech recording is noisy, accented, not in English, or has multiple talkers etc. For single talker, clean recording, American/British accent, English speech, the machine systems work quite well. Similar is the performance for talker recognition as well. What is the key behind this technology? Welcome to the world of signal processing and machine learning! .",
            "url": "https://iiscleap.github.io/coswara-blog/coswara/2020/06/20/speech_signals.html",
            "relUrl": "/coswara/2020/06/20/speech_signals.html",
            "date": " • Jun 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This blog shares our journey in the Coswara project. The project is launched by the LEAP lab, based at the Indian Institute of Science, Bangalore. .",
          "url": "https://iiscleap.github.io/coswara-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://iiscleap.github.io/coswara-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}